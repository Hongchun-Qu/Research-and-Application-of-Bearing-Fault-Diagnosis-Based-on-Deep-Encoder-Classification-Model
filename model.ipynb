{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "import os\n",
    "from scipy.fftpack import fft\n",
    "from sklearn import preprocessing  # 0-1编码\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_min(train_x, test_x):\n",
    "    # x = (x - Min) / (Max - Min)\n",
    "    scalar = preprocessing.MinMaxScaler().fit(train_x)\n",
    "    train_x = scalar.transform(train_x)\n",
    "    test_x = scalar.transform(test_x)\n",
    "    return train_x, test_x\n",
    "\n",
    "# https://blog.csdn.net/qq_27825451/article/details/88553441\n",
    "def fft_normalize(x):\n",
    "\n",
    "    fft_x = fft(x)  # 快速傅里叶变换\n",
    "    abs_x = np.abs(fft_x)  # 取复数的绝对值，即复数的模(双边频谱)\n",
    "    N = len(x)\n",
    "    abs_half_x = abs_x[range(int(N / 2))]\n",
    "    normalization_x = abs_x / N  # 归一化处理（双边频谱）\n",
    "    normalization_half_x = normalization_x[range(int(N / 2))]  # 由于对称性，只取一半区间（单边频谱）\n",
    "\n",
    "    # plt.figure()\n",
    "    # plt.plot(half_number, normalization_half_x, 'blue')\n",
    "    # plt.title('单边振幅谱(归一化)', fontsize=9, color='blue')\n",
    "    # plt.show()\n",
    "\n",
    "    return normalization_half_x\n",
    "\n",
    "# https://blog.csdn.net/sinat_24259567/article/details/93889547\n",
    "def add_noise(signal, SNR=None):\n",
    "    if SNR == None:\n",
    "        return signal\n",
    "    else:\n",
    "        noise = np.random.randn(signal.shape[0], signal.shape[1], signal.shape[2])   #产生N(0,1)噪声数据\n",
    "        # noise = np.random.randn(signal.shape[0])\n",
    "\n",
    "        noise = noise - np.mean(noise)  #均值为0\n",
    "        signal_power = np.linalg.norm(signal - signal.mean()) ** 2 / signal.size  #此处是信号的std**2\n",
    "        # linalg=linear（线性）+algebra（代数），norm则表示范数。默认求二范数\n",
    "        noise_variance = signal_power / np.power(10, (SNR / 10))    # np.power(x,y)计算x的y次方  #此处是噪声的std**2\n",
    "        noise = (np.sqrt(noise_variance) / np.std(noise)) * noise\n",
    "        signal_noise = noise + signal\n",
    "\n",
    "        # Ps = (np.linalg.norm(signal - signal.mean())) ** 2  # signal power\n",
    "        # Pn = (np.linalg.norm(signal - signal_noise)) ** 2  # noise power\n",
    "        # snr_ = 10 * np.log10(Ps / Pn)\n",
    "        # print(snr_)\n",
    "\n",
    "        return signal_noise\n",
    "\n",
    "\n",
    "'''\n",
    "所有样本都通过快速傅立叶变换（FFT）转换为频谱。为了规范化数据，通过最大-最小映射将频谱样本缩放到0到1的范围\n",
    "'''\n",
    "def cut_samples(SNR=None):\n",
    "    '''\n",
    "    每种状况得到400个样本\n",
    "    '''\n",
    "    results = np.zeros(shape=(10, 400, 600))\n",
    "    temporary_s = np.zeros(shape=(400, 1200))\n",
    "    fft_temporary_s = np.zeros(shape=(400, 600))\n",
    "\n",
    "    '''\n",
    "    下载cwru数据集\n",
    "    '''\n",
    "    files = os.listdir('C:\\\\Users\\\\LENOVO\\\\Desktop\\\\论文\\\\数据集\\\\cwru12k驱动端\\\\12k_txt_hebing')\n",
    "    files.sort()  # 对列表排序\n",
    "    for i in range(10):\n",
    "        domain = os.path.abspath(\n",
    "            r'C:\\\\Users\\\\LENOVO\\\\Desktop\\\\论文\\\\数据集\\\\cwru12k驱动端\\\\12k_txt_hebing')  # 获取文件夹的路径，此处其实没必要这么写，目的是为了熟悉os的文件夹操作\n",
    "        file = os.path.join(domain, files[i])\n",
    "        s = np.loadtxt(file)\n",
    "\n",
    "        for x in range(400):\n",
    "            temporary_s[x] = s[1000*x:1200+1000*x]\n",
    "            fft_temporary_s[x] = fft_normalize(temporary_s[x])\n",
    "            # if i==1:\n",
    "            # #     # if x==150 or x==350 :\n",
    "            # #     print(files[i])\n",
    "            # #     plt.plot(temporary_s[x])\n",
    "            # #     plt.show()\n",
    "            #\n",
    "            #     plt.plot(fft_temporary_s[x])\n",
    "            #     plt.show()\n",
    "\n",
    "            #     noise = np.random.randn(fft_temporary_s[x].shape[0])\n",
    "            #\n",
    "            #     noise = noise - np.mean(noise)  # 均值为0\n",
    "            #     signal_power = np.linalg.norm(fft_temporary_s[x] - fft_temporary_s[x].mean()) ** 2 / fft_temporary_s[x].size  # 此处是信号的std**2\n",
    "            #     #\n",
    "            #     noise_variance = signal_power / np.power(10, (SNR / 10))\n",
    "            #     noise = (np.sqrt(noise_variance) / np.std(noise)) * noise\n",
    "            #     plt.plot(noise, c='r')\n",
    "            #     plt.show()\n",
    "        results[i] = fft_temporary_s\n",
    "\n",
    "    results = add_noise(results, SNR)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 划分训练集和测试集\n",
    "def make_datasets(results):\n",
    "    '''输入10*400*1200的原始样本'''\n",
    "    train_x = np.zeros(shape=(10, 320, 600))\n",
    "    train_y = np.zeros(shape=(10, 320, 1))\n",
    "    test_x = np.zeros(shape=(10, 80, 600))\n",
    "    test_y = np.zeros(shape=(10, 80, 1))\n",
    "\n",
    "    for i in range(10):\n",
    "        s = results[i]\n",
    "        # 打乱顺序\n",
    "        index_s = [a for a in range(len(s))]\n",
    "        shuffle(index_s)\n",
    "        s = s[index_s]\n",
    "        # 对每种类型都划分训练集和测试集\n",
    "        train_x[i] = s[:320]\n",
    "        test_x[i] = s[320:400]\n",
    "\n",
    "        # 填写标签\n",
    "        train_y[i, :] = i\n",
    "        test_y[i, :] = i\n",
    "\n",
    "    #将十种类型的训练集和测试集分别合并并打乱\n",
    "    x1 = train_x[0]\n",
    "    y1 = train_y[0]\n",
    "    x2 = test_x[0]\n",
    "    y2 = test_y[0]\n",
    "    for i in range(9):\n",
    "        x1 = np.row_stack((x1, train_x[i + 1]))\n",
    "        x2 = np.row_stack((x2, test_x[i + 1]))\n",
    "\n",
    "        y1 = np.row_stack((y1, train_y[i + 1]))\n",
    "        y2 = np.row_stack((y2, test_y[i + 1]))\n",
    "\n",
    "\n",
    "    index_x1 = [i for i in range(len(x1))]\n",
    "    index_x2 = [i for i in range(len(x2))]\n",
    "\n",
    "    shuffle(index_x1)\n",
    "    shuffle(index_x2)\n",
    "\n",
    "    x1 = x1[index_x1]\n",
    "    y1 = y1[index_x1]\n",
    "    x2 = x2[index_x2]\n",
    "    y2 = y2[index_x2]\n",
    "\n",
    "    x1, x2 = max_min(x1, x2)\n",
    "    y1 = y1.astype(np.int64)\n",
    "    y2 = y2.astype(np.int64)\n",
    "\n",
    "\n",
    "    return x1, y1, x2, y2   #分别代表：训练集样本，训练集标签，测试集样本，测试集标签\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DHA_DDA():\n",
    "    def __init__(self, unit1, unit2, unit3, model_name='dha_dda', train_step=10, fault_num=10,\n",
    "                 batch_size=500, lr=0.8, R_constant = 1.0, P=1.0, P_R2Cm=1.0, P_class=0.0001):\n",
    "        \"\"\"\n",
    "        :param DataFram:  数据集\n",
    "        :param train_step:训练步数\n",
    "        :param R_constant:初始化半径\n",
    "        :param lr:        优化器学习率\n",
    "        :param P:         每个球体的惩罚系数\n",
    "        :param P_R2Cm:    球体之间的惩罚系数\n",
    "        :param P_class:   加速收敛系数\n",
    "        \"\"\"\n",
    "        self.train_step = train_step\n",
    "        self.lr = lr\n",
    "        self.P = P\n",
    "        self.P_R2Cm = P_R2Cm\n",
    "        self.P_class = P_class\n",
    "        self.R_constant = R_constant\n",
    "\n",
    "        # 编码器部分\n",
    "        self.lambda1 = 1e-4  # 权重衰减项\n",
    "        self.noise_factor = 0.05\n",
    "        self.keep_prop = 0.9  # 数据保留的比例， 用来防止过拟合\n",
    "        self.unit1 = unit1\n",
    "        self.unit2 = unit2\n",
    "        self.unit3 = unit3\n",
    "        self.batch_size = batch_size\n",
    "        self.pop = np.array([600, self.unit1, self.unit2, self.unit3, 1])\n",
    "        self.model_name = model_name\n",
    "        self.dha_dda = tf.Graph()\n",
    "        self.fault_num = fault_num\n",
    "\n",
    "        self.data_space = {}\n",
    "        self.index_space = {}\n",
    "        self.activations = []\n",
    "        self.names = {}\n",
    "        self.split_num = []  # 每个类别的数据的个数\n",
    "\n",
    "        # lrp\n",
    "        self.weights = []\n",
    "        self.activations = []\n",
    "\n",
    "\n",
    "    # 编码器的损失函数\n",
    "    def encoder_loss(self, data, outputdata, hidden_w, output_w):\n",
    "\n",
    "        error_loss = tf.reduce_mean(tf.square(outputdata - data))\n",
    "        tf.add_to_collection(\"losses\", error_loss)\n",
    "        # 参数L2正则化\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(self.lambda1)\n",
    "        regularization = regularizer(hidden_w) + regularizer(output_w)\n",
    "        tf.add_to_collection(\"losses\", regularization)\n",
    "        # get_collection函数获取指定集合中的所有个体，这里是获取所有损失值，并在 add_n() 函数中进行加和运算\n",
    "        loss_dae = tf.add_n(tf.get_collection(\"losses\"))\n",
    "\n",
    "        optimizer_dae = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss_dae)\n",
    "        return loss_dae, optimizer_dae\n",
    "\n",
    "\n",
    "    # 在圆心外惩罚\n",
    "    def g1n_term(self, var, center, Rm):\n",
    "        g1n = tf.linalg.norm(var - center, axis=1) - Rm  # 点到球心的距离 - 半径 = 点到球的距离\n",
    "        g1n_max = tf.clip_by_value(g1n, 0, 1e10)\n",
    "        penalty = tf.reduce_mean(g1n_max)  # if res>0, penalty = res else penalty = 0\n",
    "        return penalty\n",
    "\n",
    "    # 在圆心内惩罚\n",
    "    def g2n_term(self, var, center, Rm):\n",
    "        g2n = Rm - tf.linalg.norm(var - center, axis=1)  # 半径 - 点到球心的距离 = 点到球边缘的距离\n",
    "        g2n_max = tf.clip_by_value(g2n, 0, 1e10)\n",
    "        penalty = tf.reduce_mean(g2n_max)  # if res>0,penalty = res else penalty = 0\n",
    "        return penalty\n",
    "\n",
    "\n",
    "    # DHA分类器的损失函数\n",
    "    def dha_loss(self):\n",
    "        with self.dha_dda.as_default():\n",
    "            with tf.name_scope(self.model_name):\n",
    "\n",
    "                with tf.name_scope('loss_pow'):\n",
    "                    split_list = np.arange(0, len(self.split_num))\n",
    "                    for i in range(len(self.split_num)):\n",
    "                        with tf.name_scope('loss_pow{}'.format(i)):\n",
    "                            split_list_ = np.delete(split_list, i)\n",
    "                            if len(split_list_) == 1:\n",
    "                                U_ = self.names['U{}'.format(split_list_[0])]\n",
    "                            else:\n",
    "                                U_ = tf.concat([self.names['U{}'.format(j)] for j in split_list_], 0)\n",
    "\n",
    "                            g1n1 = self.g1n_term(self.names['U{}'.format(i)],  # layer：weight * v + bias\n",
    "                                                 self.names['Cm{}'.format(i)],  # 球心\n",
    "                                                 self.names['R{}'.format(i)])  # 半径\n",
    "                            g2n1 = self.g2n_term(U_,\n",
    "                                                 self.names['Cm{}'.format(i)],\n",
    "                                                 self.names['R{}'.format(i)])\n",
    "\n",
    "                            Rn = tf.where(tf.greater(np.float64(0), self.names['R{}'.format(i)]),\n",
    "                                          self.names['R{}'.format(i)], 0)\n",
    "                            self.names['loss{}_pow'.format(i)] = tf.pow(g1n1, 2) + tf.pow(g2n1, 2) + tf.pow(Rn, 2)\n",
    "\n",
    "                    loss_pow = 0\n",
    "                    for i in range(len(self.split_num)):\n",
    "                        loss_pow = loss_pow + self.names['loss{}_pow'.format(i)]\n",
    "\n",
    "\n",
    "                with tf.name_scope('lossR2Cm'):\n",
    "                    combine = list(itertools.combinations(np.arange(0, len(self.split_num)).tolist(), 2))\n",
    "                    loss_R2Cm = 0\n",
    "                    for i in combine:\n",
    "                        with tf.name_scope('lossR2Cm{}{}'.format(i[0], i[1])):\n",
    "                            Cm_normal = (self.names['R{}'.format(i[0])] + self.names['R{}'.format(i[1])]) \\\n",
    "                                        - tf.linalg.norm(self.names['Cm{}'.format(i[0])] - self.names['Cm{}'.format(i[1])])\n",
    "                            loss_R2Cm = loss_R2Cm + tf.where(tf.greater(Cm_normal, 0), Cm_normal, 0)\n",
    "\n",
    "\n",
    "                with tf.name_scope('loss_class'):\n",
    "                    loss_class = 0.0\n",
    "                    for i in range(len(self.split_num)):\n",
    "                        loss_class = loss_class + tf.linalg.norm(self.names['U{}'.format(i)] - self.names['Cm{}'.format(i)])\n",
    "\n",
    "\n",
    "                with tf.name_scope('loss_all'):\n",
    "                    self.loss_all = self.P * (loss_pow) + self.P_R2Cm * loss_R2Cm + self.P_class * loss_class\n",
    "\n",
    "            return self.loss_all\n",
    "\n",
    "\n",
    "    def gen_model(self, data_noisy, train_label):\n",
    "        with self.dha_dda.as_default():\n",
    "            with tf.name_scope(self.model_name):\n",
    "\n",
    "                for class_i in range(self.fault_num):\n",
    "                    i_d = np.argwhere(train_label == class_i)\n",
    "                    i_d = i_d[:,0]\n",
    "                    self.split_num.append(len(i_d))\n",
    "                    self.data_space['data{}'.format(class_i)] = data_noisy[i_d]\n",
    "                    self.index_space['index{}'.format(class_i)] = i_d  # 二维\n",
    "\n",
    "\n",
    "                with tf.name_scope('InitVariable'):\n",
    "                    # 动态变量名设置\n",
    "                    for i in range(len(self.split_num)):\n",
    "                        with tf.name_scope('V{}'.format(i)):\n",
    "                            self.names['V{}'.format(i)] = tf.placeholder(\n",
    "                                tf.float64, shape=[None, data_noisy.shape[1]], name='Input{}'.format(i))\n",
    "\n",
    "\n",
    "                    with tf.name_scope('weight'):\n",
    "                        self.hidden_w1 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[0], self.pop[1]]),\n",
    "                                                        trainable=True), tf.float64, name='hidden_w1')  # float32\n",
    "                        output_w1 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[1], self.pop[0]]),\n",
    "                                                        trainable=True), tf.float64, name='output_w1')\n",
    "                        self.hidden_w2 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[1], self.pop[2]]),\n",
    "                                                        trainable=True), tf.float64, name='hidden_w2')  # float32\n",
    "                        output_w2 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[2], self.pop[1]]),\n",
    "                                                        trainable=True), tf.float64, name='output_w2')\n",
    "                        self.hidden_w3 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[2], self.pop[3]]),\n",
    "                                                        trainable=True), tf.float64, name='hidden_w3')  # float32\n",
    "                        out_w3 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[3], self.pop[2]]),\n",
    "                                                     trainable=True), tf.float64, name='out_w3')\n",
    "\n",
    "                        self.weights = [self.hidden_w1, self.hidden_w2, self.hidden_w3]\n",
    "\n",
    "\n",
    "                    with tf.name_scope('bias'):\n",
    "                        self.hidden_b1 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[4], self.pop[1]]),\n",
    "                                                        trainable=True), tf.float64, name='hidden_b1')\n",
    "                        output_b1 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[4], self.pop[0]]),\n",
    "                                                        trainable=True), tf.float64, name='output_b1')\n",
    "                        self.hidden_b2 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[4], self.pop[2]]),\n",
    "                                                        trainable=True), tf.float64, name='hidden_b2')\n",
    "                        output_b2 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[4], self.pop[1]]),\n",
    "                                                        trainable=True), tf.float64, name='output_b2')\n",
    "                        self.hidden_b3 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[4], self.pop[3]]),\n",
    "                                                        trainable=True), tf.float64, name='hidden_b3')\n",
    "                        out_b3 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[4], self.pop[2]]),\n",
    "                                                     trainable=True), tf.float64, name='out_b3')\n",
    "\n",
    "\n",
    "                    # R_constant:初始化半径\n",
    "                    with tf.name_scope('R'):\n",
    "                        for i in range(len(self.split_num)):\n",
    "                            self.names['R{}'.format(i)] = tf.Variable(initial_value=self.R_constant,\n",
    "                                                                 dtype=tf.float64, name='R{}'.format(i), trainable=True)\n",
    "\n",
    "\n",
    "                with tf.name_scope('encoder_layer'):\n",
    "                    for i in range(len(self.split_num)):\n",
    "\n",
    "                        hidden1 = tf.nn.sigmoid(tf.add(tf.matmul(self.names['V{}'.format(i)], self.hidden_w1), self.hidden_b1))\n",
    "                        output_ae1 = tf.nn.sigmoid(tf.add(tf.matmul(hidden1, output_w1), output_b1))\n",
    "                        hidden2 = tf.nn.sigmoid(tf.add(tf.matmul(hidden1, self.hidden_w2), self.hidden_b2))\n",
    "                        output_ae2 = tf.nn.sigmoid(tf.add(tf.matmul(hidden2, output_w2), output_b2))\n",
    "                        self.names['U{}'.format(i)] = tf.nn.sigmoid(tf.add(tf.matmul(hidden2, self.hidden_w3), self.hidden_b3))\n",
    "                        out_ae3 = tf.nn.sigmoid(tf.add(tf.matmul(self.names['U{}'.format(i)], out_w3), out_b3))\n",
    "\n",
    "                        # 逐层训练\n",
    "                        self.loss_ae1, self.optimizer_ae1 = self.encoder_loss(\n",
    "                            self.names['V{}'.format(i)], output_ae1, self.hidden_w1,  output_w1)\n",
    "\n",
    "                        self.loss_ae2, self.optimizer_ae2 = self.encoder_loss(\n",
    "                            hidden1, output_ae2, self.hidden_w2, output_w2)\n",
    "\n",
    "                        self.loss_ae3, self.optimizer_ae3 = self.encoder_loss(\n",
    "                            hidden2, out_ae3, self.hidden_w3, out_w3)\n",
    "\n",
    "                    self.activations = [self.names['V{}'.format(i)], hidden1, hidden2, self.names['U{}'.format(i)]]\n",
    "\n",
    "\n",
    "                # 球心\n",
    "                with tf.name_scope('circle'):\n",
    "                    for i in range(len(self.split_num)):\n",
    "                        self.names['Cm{}'.format(i)] = tf.reduce_mean(self.names['U{}'.format(i)], axis=0)\n",
    "\n",
    "\n",
    "    def fit(self, train_data, train_label, tree_i):\n",
    "        '''\n",
    "        :param train_data: 训练数据集\n",
    "        :param train_label: 训练集的标签，输入的标签应该是一位的，即1,2, 3......\n",
    "        :return:\n",
    "        '''\n",
    "        with self.dha_dda.as_default():\n",
    "            with tf.name_scope(self.model_name):\n",
    "\n",
    "                data_noisy = train_data + self.noise_factor * np.random.normal(loc=0.0, scale=1.0, size=train_data.shape)\n",
    "                self.gen_model(data_noisy, train_label)\n",
    "                dha_loss = self.dha_loss()\n",
    "\n",
    "\n",
    "                with tf.name_scope('Optimizer'):\n",
    "                    learning_rate = tf.train.exponential_decay(self.lr, self.train_step, decay_steps=100,\n",
    "                                                               decay_rate=0.8)\n",
    "                    train_op = tf.train.AdamOptimizer(learning_rate).minimize(dha_loss)\n",
    "\n",
    "\n",
    "                for weight in self.weights:\n",
    "                    tf.add_to_collection('LRP_DDA_weights'+str(tree_i), weight)\n",
    "\n",
    "                for act in self.activations:\n",
    "                    tf.add_to_collection('LRP_DDA_activations'+str(tree_i), act)\n",
    "\n",
    "                saver = tf.train.Saver()\n",
    "                with tf.Session(graph=self.dha_dda) as sess:\n",
    "                    sess.run(tf.global_variables_initializer())\n",
    "                    # writer = tf.summary.FileWriter(\"demo_class\", sess.graph)\n",
    "                    # writer.close()\n",
    "\n",
    "                    feed_dicts = {}\n",
    "                    for i in range(len(self.split_num)):\n",
    "                        feed_dicts[self.names['V{}'.format(i)]] = self.data_space['data{}'.format(i)]\n",
    "\n",
    "                    sess.run([self.loss_ae1, self.optimizer_ae1, self.loss_ae2, self.optimizer_ae2,\n",
    "                              self.loss_ae3, self.optimizer_ae3], feed_dict=feed_dicts)\n",
    "\n",
    "                    self.loss_list = []\n",
    "                    self.R_history = []\n",
    "\n",
    "                    print('Enter train the Space........')\n",
    "                    t1_ = time.time()\n",
    "                    feed_batch_dicts = {}\n",
    "\n",
    "                    for j in range(self.train_step):\n",
    "                        for batch_i in range(int(np.ceil(len(train_data) / self.batch_size))):\n",
    "                            for i in range(len(self.split_num)):\n",
    "                                feed_batch_dicts[self.names['V{}'.format(i)]] = random.choices(\n",
    "                                    self.data_space['data{}'.format(i)], k=int(self.batch_size/self.fault_num))\n",
    "                            _ = sess.run(train_op, feed_dict=feed_batch_dicts)\n",
    "                            loss = sess.run(self.loss_all, feed_dict=feed_batch_dicts)\n",
    "                        self.loss_list.append(loss)\n",
    "                        R = sess.run([self.names['R{}'.format(i)] for i in range(len(self.split_num))])  # 半径\n",
    "                        self.R_history.append(R)\n",
    "                        print(\" Epoch\", j, \": loss : \", loss)\n",
    "\n",
    "                        # if loss < 1.0:\n",
    "                        #     break\n",
    "\n",
    "                    t2_ = time.time()\n",
    "                    print('训练时间：%.2f s'% (t2_ - t1_))\n",
    "\n",
    "\n",
    "                    self.R_list = []\n",
    "                    self.circle = []\n",
    "                    self.circle = sess.run([self.names['Cm{}'.format(i)] for i in range(self.fault_num)], feed_dict=feed_dicts)\n",
    "                    self.R_list = sess.run([self.names['R{}'.format(i)] for i in range(self.fault_num)])\n",
    "\n",
    "\n",
    "                    w1, w2, w3 = sess.run([self.hidden_w1, self.hidden_w2, self.hidden_w3])\n",
    "                    b1, b2, b3 = sess.run([self.hidden_b1, self.hidden_b2, self.hidden_b3])\n",
    "                    self.encoder_weight = [w1, w2, w3]\n",
    "                    self.encoder_bias = [b1, b2, b3]\n",
    "\n",
    "\n",
    "                    u_list = sess.run([self.names['U{}'.format(i)] for i in range(self.fault_num)], feed_dict=feed_dicts)\n",
    "                    DHA_pre = np.zeros(shape=[train_label.shape[0], 1])\n",
    "                    for i in range(len(self.split_num)): # 每个类的数据\n",
    "                        var = u_list[i]\n",
    "                        dis = np.zeros(shape=[var.shape[0], len(self.split_num)])\n",
    "                        for j in range(len(self.split_num)):  # 和每个球心求距离\n",
    "\n",
    "                            center = tf.reshape(tf.tile(self.circle[j], [self.split_num[i]]),\n",
    "                                                [self.split_num[i], len(self.circle[j])])   # tf.tile 对张量进行复制\n",
    "                            dis_c = sess.run(tf.linalg.norm(var - center, axis=1))\n",
    "                            dis_c = dis_c / self.R_list[j]\n",
    "                            dis[:, j] = dis_c\n",
    "\n",
    "                        tmp_id = self.index_space['index{}'.format(i)]\n",
    "                        DHA_pre[tmp_id] = np.argmin(dis, axis=1).reshape(-1, 1)\n",
    "\n",
    "                    DHA_pre = DHA_pre.astype(np.int64)\n",
    "                    acc = tf.reduce_mean(tf.cast(tf.equal(DHA_pre, train_label), tf.float32))\n",
    "                    acc = sess.run(acc)\n",
    "\n",
    "                    # 画r和loss的变化曲线图\n",
    "                    dir1 = './DHA_DDA_jnu/'\n",
    "                    if not os.path.exists(dir1):\n",
    "                        os.mkdir(dir1)\n",
    "                    # picturesdir = './DHA_DDA/pictures/'\n",
    "                    # if not os.path.exists(picturesdir):\n",
    "                    #     os.mkdir(picturesdir)\n",
    "                    # fig1 = Visualization(train_label, DHA_pre, picturesdir)\n",
    "                    # fig1.plot_R(self.R_history, self.split_num)\n",
    "                    # fig1.plot_loss(self.loss_list)\n",
    "\n",
    "                    saver.save(sess, \"./DHA_DDA_jnu/model_\" + str(tree_i))  # 用于LRP\n",
    "                    sess.close()\n",
    "                    return DHA_pre, self.circle, self.R_list, self.encoder_weight, self.encoder_bias, loss, acc\n",
    "\n",
    "\n",
    "\n",
    "    def test(self, testdata, test_label, Cm, R, w, b, test=False):\n",
    "        with tf.Session() as sess:\n",
    "            # data_noisy = testdata + self.noise_factor * np.random.normal(loc=0.0, scale=1.0, size=testdata.shape)\n",
    "            hidden1 = tf.nn.sigmoid(tf.add(tf.matmul(testdata, w[0]), b[0]))\n",
    "            hidden2 = tf.nn.sigmoid(tf.add(tf.matmul(hidden1, w[1]), b[1]))\n",
    "            space_test = tf.nn.sigmoid(tf.add(tf.matmul(hidden2, w[2]), b[2]))\n",
    "\n",
    "            m = space_test.shape[0]\n",
    "            test_pre = np.zeros(shape=[testdata.shape[0],  self.fault_num])\n",
    "            for i in range(len(self.split_num)):\n",
    "\n",
    "                center = tf.reshape(tf.tile(Cm[i], [m]), [m, len(Cm[i])])  # tf.tile 对张量进行复制\n",
    "                distance = sess.run(tf.linalg.norm(space_test - center, axis=1))\n",
    "                distance /= R[i]\n",
    "                test_pre[:, i] = distance\n",
    "\n",
    "            test_pre = np.argmin(test_pre, axis=1).astype(np.int64).reshape(-1, 1)\n",
    "            acc = tf.reduce_mean(tf.cast(tf.equal(test_pre, test_label), tf.float32))\n",
    "            acc = sess.run(acc)\n",
    "            print(acc)\n",
    "\n",
    "            picturesdir = './DHA_DDA_jnu/pictures/'\n",
    "            if not os.path.exists(picturesdir):\n",
    "                os.mkdir(picturesdir)\n",
    "\n",
    "            acc = accuracy_score(test_label, test_pre)\n",
    "            print('accuracy：', acc)\n",
    "            rec_w = recall_score(test_label, test_pre, average='macro')\n",
    "            print('recall score (macro)：', rec_w)\n",
    "            f1_w = f1_score(test_label, test_pre, average='macro')\n",
    "            print('f1_score (macro)：', f1_w)\n",
    "            precision_w = precision_score(test_label, test_pre, average='macro')\n",
    "            print('precision_score (macro)：', precision_w)\n",
    "\n",
    "            if test == True:\n",
    "                fig2 = Visualization(test_label, test_pre, picturesdir)\n",
    "                acc = fig2.plot_confusion_matrix('DHA_DDA test HAR Confusion Matrix')  # 画混淆矩阵\n",
    "                space_test = sess.run(space_test)\n",
    "                fig2.pca_2D(space_test)\n",
    "                fig2.tsne_3d(space_test)\n",
    "                return acc, test_pre, space_test\n",
    "\n",
    "            # fig2 = Visualization(test_label, test_pre, picturesdir)\n",
    "            # acc = fig2.plot_confusion_matrix('DHA_DDA test HAR Confusion Matrix')  # 画混淆矩阵\n",
    "\n",
    "            # fig2.plot_roc('roc')\n",
    "            # sess.close()\n",
    "            else:\n",
    "                return acc, test_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DHA_CDA():\n",
    "    def __init__(self, unit1, unit2, unit3, model_name='dha_cda', train_step=10, fault_num=10,\n",
    "                 batch_size=500, lr=0.8, R_constant=1.0, P=1.0, P_R2Cm=1.0, P_class=0.0001):\n",
    "        \"\"\"\n",
    "        :param DataFram:  数据集\n",
    "        :param train_step:训练步数\n",
    "        :param R_constant:初始化半径\n",
    "        :param lr:        优化器学习率\n",
    "        :param P:         每个球体的惩罚系数\n",
    "        :param P_R2Cm:    球体之间的惩罚系数\n",
    "        :param P_class:   加速收敛系数\n",
    "        \"\"\"\n",
    "        self.train_step = train_step\n",
    "        self.lr = lr\n",
    "        self.P = P\n",
    "        self.P_R2Cm = P_R2Cm\n",
    "        self.P_class = P_class\n",
    "        self.R_constant = R_constant\n",
    "\n",
    "\n",
    "        # 编码器部分\n",
    "        self.delta1 = 1e-3  # 雅克比惩罚项系数\n",
    "\n",
    "        self.unit1 = unit1\n",
    "        self.unit2 = unit2\n",
    "        self.unit3 = unit3\n",
    "        self.batch_size = batch_size\n",
    "        self.pop = np.array([600, self.unit1, self.unit2, self.unit3, 1])\n",
    "        self.model_name = model_name\n",
    "        self.dha_cda = tf.Graph()\n",
    "        self.fault_num = fault_num\n",
    "\n",
    "        self.data_space = {}\n",
    "        self.index_space = {}\n",
    "        self.names = {}\n",
    "        self.split_num = []  # 每个类别的数据的个数\n",
    "\n",
    "        # lrp\n",
    "        self.weights = []\n",
    "        self.activations = []\n",
    "\n",
    "\n",
    "    # 编码器的损失函数\n",
    "    def encoder_loss(self, data, outputdata, hidden_w, hidden):\n",
    "\n",
    "        error_loss = tf.reduce_mean(tf.square(outputdata - data))\n",
    "        # 雅克比矩阵 https://wiseodd.github.io/techblog/2016/12/05/contractive-autoencoder/\n",
    "        J_w = tf.transpose(hidden_w)\n",
    "        d_sigmoid = hidden * (1 - hidden)\n",
    "        contractive = self.delta1 * tf.reduce_sum(d_sigmoid ** 2 * tf.reduce_sum(J_w ** 2, axis=1), axis=1)\n",
    "\n",
    "        loss_ae = error_loss + contractive\n",
    "        optimizer_ae = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss_ae, var_list=tf.trainable_variables())\n",
    "\n",
    "        return loss_ae, optimizer_ae\n",
    "\n",
    "\n",
    "    # 在圆心外惩罚\n",
    "    # tf.clip_by_value(A, min, max)：输入一个张量A，把A中的每一个元素的值都压缩在min和max之间。\n",
    "    # 小于min的让它等于min，大于max的元素的值等于max。\n",
    "    def g1n_term(self, var, center, Rm):\n",
    "        g1n = tf.linalg.norm(var - center, axis=1) - Rm  # 点到球心的距离 - 半径 = 点到球的距离\n",
    "        g1n_max = tf.clip_by_value(g1n, 0, 1e10)\n",
    "        penalty = tf.reduce_mean(g1n_max)  # if res>0, penalty = res else penalty = 0\n",
    "        return penalty\n",
    "\n",
    "    # 在圆心内惩罚\n",
    "    def g2n_term(self, var, center, Rm):\n",
    "        g2n = Rm - tf.linalg.norm(var - center, axis=1)  # 半径 - 点到球心的距离 = 点到球边缘的距离\n",
    "        g2n_max = tf.clip_by_value(g2n, 0, 1e10)\n",
    "        penalty = tf.reduce_mean(g2n_max)  # if res>0,penalty = res else penalty = 0\n",
    "        return penalty\n",
    "\n",
    "\n",
    "    # DHA分类器的损失函数\n",
    "    def dha_loss(self):\n",
    "        with self.dha_cda.as_default():\n",
    "            with tf.name_scope(self.model_name):\n",
    "\n",
    "                with tf.name_scope('loss_pow'):\n",
    "                    # len(self.split_num) = 3\n",
    "                    split_list = np.arange(0, len(self.split_num))\n",
    "                    for i in range(len(self.split_num)):\n",
    "                        with tf.name_scope('loss_pow{}'.format(i)):\n",
    "                            split_list_ = np.delete(split_list, i)  # 按行删除，删除第i+1个数，因为从0开始\n",
    "                            if len(split_list_) == 1:\n",
    "                                U_ = self.names['U{}'.format(split_list_[0])]\n",
    "                            else:\n",
    "                                U_ = tf.concat([self.names['U{}'.format(j)] for j in split_list_], 0)\n",
    "\n",
    "                            g1n1 = self.g1n_term(self.names['U{}'.format(i)],  # layer：weight * v + bias\n",
    "                                                 self.names['Cm{}'.format(i)],  # 球心\n",
    "                                                 self.names['R{}'.format(i)])  # 半径\n",
    "                            g2n1 = self.g2n_term(U_,\n",
    "                                                 self.names['Cm{}'.format(i)],\n",
    "                                                 self.names['R{}'.format(i)])\n",
    "\n",
    "                            # 如果 半径 Ri 小于 0 的话，就令Rn = Ri, 否则的话，令Rn = 0\n",
    "                            Rn = tf.where(tf.greater(np.float64(0), self.names['R{}'.format(i)]),\n",
    "                                          self.names['R{}'.format(i)], 0)\n",
    "                            # loss = 该类中所有样本的欧式距离 + P * { 本类样本 + 非本类样本 }\n",
    "                            # P: 每个球体的惩罚系数 = 1\n",
    "                            self.names['loss{}_pow'.format(i)] = tf.pow(g1n1, 2) + tf.pow(g2n1, 2) + tf.pow(Rn, 2)\n",
    "\n",
    "                    loss_pow = 0\n",
    "                    for i in range(len(self.split_num)):\n",
    "                        loss_pow = loss_pow + self.names['loss{}_pow'.format(i)]\n",
    "\n",
    "                        # tf.greater(a,b) 功能：通过比较a、b两个值的大小来输出对错。\n",
    "                        # 比如：当a=4，b=3时，输出结果为：true；当a=2，b=3时，输出结果为：false。\n",
    "\n",
    "                        # tf.where的作用是根据condition,返回相对应的x或y,返回值是一个tf.bool类型的Tensor\n",
    "                        # 比如：A =tf.where(False,123,321) 输出：A = Tensor(\"Select:0\", shape=(), dtype=int32)\n",
    "                        # 然后sess.run(A) 可得 A = 321\n",
    "                        # sess.run(tf.where(True,123,321)) = 123\n",
    "\n",
    "                # tolist():转换为list\n",
    "                # itertools.combinations：\n",
    "                # 创建一个迭代器，返回iterable中所有长度为r的子序列，返回的子序列中的项按输入iterable中的顺序排序(不带重复)\n",
    "                with tf.name_scope('lossR2Cm'):\n",
    "                    combine = list(itertools.combinations(np.arange(0, len(self.split_num)).tolist(), 2))\n",
    "                    loss_R2Cm = 0\n",
    "                    for i in combine:\n",
    "                        with tf.name_scope('lossR2Cm{}{}'.format(i[0], i[1])):\n",
    "                            # Cm_normal：两个球的半径的和 - 两个球的中心差的线性范数\n",
    "                            Cm_normal = (self.names['R{}'.format(i[0])] + self.names['R{}'.format(i[1])]) \\\n",
    "                                        - tf.linalg.norm(self.names['Cm{}'.format(i[0])] - self.names['Cm{}'.format(i[1])])\n",
    "                            loss_R2Cm = loss_R2Cm + tf.where(tf.greater(Cm_normal, 0), Cm_normal, 0)\n",
    "\n",
    "                with tf.name_scope('loss_class'):\n",
    "                    loss_class = 0.0\n",
    "                    for i in range(len(self.split_num)):\n",
    "                        loss_class = loss_class + tf.linalg.norm(self.names['U{}'.format(i)] - self.names['Cm{}'.format(i)])\n",
    "\n",
    "                with tf.name_scope('loss_all'):\n",
    "                    self.loss_all = self.P * (loss_pow) + self.P_R2Cm * loss_R2Cm + self.P_class * loss_class\n",
    "\n",
    "            return self.loss_all\n",
    "\n",
    "\n",
    "    def gen_model(self, train_data, train_label):\n",
    "        with self.dha_cda.as_default():\n",
    "            with tf.name_scope(self.model_name):\n",
    "\n",
    "                for class_i in range(self.fault_num):\n",
    "                    i_d = np.argwhere(train_label == class_i)\n",
    "                    i_d = i_d[:, 0]\n",
    "                    self.split_num.append(len(i_d))\n",
    "                    self.data_space['data{}'.format(class_i)] = train_data[i_d]\n",
    "                    self.index_space['index{}'.format(class_i)] = i_d  # 二维\n",
    "\n",
    "                # names的类型是dict：3，即建立一个字典用来存输入的数据 ‘V0’：shape=(?,13)\n",
    "                with tf.name_scope('InitVariable'):\n",
    "                    # 动态变量名设置\n",
    "                    for i in range(len(self.split_num)):\n",
    "                        # self.split_num是一个列表：元素为每一类的数据个数 ，即  [52, 63, 45]\n",
    "                        # len(self.split_num)为类别数\n",
    "                        with tf.name_scope('V{}'.format(i)):  # self.x_w = data.shape[1] 即列数\n",
    "                            self.names['V{}'.format(i)] = tf.placeholder(\n",
    "                                tf.float64, shape=[None, train_data.shape[1]], name='Input{}'.format(i))\n",
    "\n",
    "\n",
    "                    with tf.name_scope('weight'):\n",
    "                        self.hidden_w1 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[0], self.pop[1]]),\n",
    "                                                        trainable=True), tf.float64, name='hidden1_w')  # float32\n",
    "                        output_w1 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[1], self.pop[0]]),\n",
    "                                                        trainable=True), tf.float64, name='output1_w')\n",
    "                        self.hidden_w2 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[1], self.pop[2]]),\n",
    "                                                        trainable=True), tf.float64, name='hidden2_w')  # float32\n",
    "                        output_w2 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[2], self.pop[1]]),\n",
    "                                                        trainable=True), tf.float64, name='output2_w')\n",
    "                        self.hidden_w3 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[2], self.pop[3]]),\n",
    "                                                        trainable=True), tf.float64, name='hidden3_w')  # float32\n",
    "                        out_w3 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[3], self.pop[2]]),\n",
    "                                                     trainable=True), tf.float64, name='out3_w')\n",
    "\n",
    "                        self.weights = [self.hidden_w1, self.hidden_w2, self.hidden_w3]\n",
    "\n",
    "\n",
    "                    with tf.name_scope('bias'):\n",
    "                        self.hidden_b1 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[4], self.pop[1]]),\n",
    "                                                        trainable=True), tf.float64, name='hidden1_b')\n",
    "                        output_b1 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[4], self.pop[0]]),\n",
    "                                                        trainable=True), tf.float64, name='output1_b')\n",
    "                        self.hidden_b2 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[4], self.pop[2]]),\n",
    "                                                        trainable=True), tf.float64, name='hidden2_b')\n",
    "                        output_b2 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[4], self.pop[1]]),\n",
    "                                                        trainable=True), tf.float64, name='output2_b')\n",
    "                        self.hidden_b3 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[4], self.pop[3]]),\n",
    "                                                        trainable=True), tf.float64, name='hidden3_b')\n",
    "                        out_b3 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[4], self.pop[2]]),\n",
    "                                                     trainable=True), tf.float64, name='out3_b')\n",
    "\n",
    "\n",
    "                    # R_constant:初始化半径\n",
    "                    # 每个类的球的半径\n",
    "                    with tf.name_scope('R'):\n",
    "                        for i in range(len(self.split_num)):\n",
    "                            self.names['R{}'.format(i)] = tf.Variable(initial_value=self.R_constant,\n",
    "                                                                 dtype=tf.float64, name='R{}'.format(i), trainable=True)\n",
    "\n",
    "                with tf.name_scope('encoder_layer'):\n",
    "\n",
    "                    for i in range(len(self.split_num)):\n",
    "\n",
    "                        hidden1 = tf.nn.sigmoid(tf.add(tf.matmul(self.names['V{}'.format(i)], self.hidden_w1), self.hidden_b1))\n",
    "                        output_ae1 = tf.nn.sigmoid(tf.add(tf.matmul(hidden1, output_w1), output_b1))\n",
    "                        hidden2 = tf.nn.sigmoid(tf.add(tf.matmul(hidden1, self.hidden_w2), self.hidden_b2))\n",
    "                        output_ae2 = tf.nn.sigmoid(tf.add(tf.matmul(hidden2, output_w2), output_b2))\n",
    "                        self.names['U{}'.format(i)] = tf.nn.sigmoid(tf.add(tf.matmul(hidden2, self.hidden_w3), self.hidden_b3))\n",
    "                        out_ae3 = tf.nn.sigmoid(tf.add(tf.matmul(self.names['U{}'.format(i)], out_w3), out_b3))\n",
    "\n",
    "                        # 逐层训练\n",
    "                        self.loss_cae1, self.optimizer_cae1 = self.encoder_loss(\n",
    "                            self.names['V{}'.format(i)], output_ae1, self.hidden_w1, hidden1)\n",
    "\n",
    "                        self.loss_cae2, self.optimizer_cae2 = self.encoder_loss(\n",
    "                            hidden1, output_ae2, self.hidden_w2, hidden2)\n",
    "\n",
    "                        self.loss_cae3, self.optimizer_cae3 = self.encoder_loss(\n",
    "                            hidden2, out_ae3, self.hidden_w3, self.names['U{}'.format(i)])\n",
    "\n",
    "                    self.activations = [self.names['V{}'.format(i)], hidden1, hidden2, self.names['U{}'.format(i)]]\n",
    "\n",
    "                # 球心\n",
    "                # 对每个类的 每个 layer 求均值\n",
    "                # axis=0，表示对第一维度（行）减少，减少行的方法是对所有列求平均，即在行上压缩减少为一行。\n",
    "                with tf.name_scope('circle'):\n",
    "                    for i in range(len(self.split_num)):\n",
    "                        self.names['Cm{}'.format(i)] = tf.reduce_mean(self.names['U{}'.format(i)], axis=0)\n",
    "\n",
    "\n",
    "    def fit(self, train_data, train_label, tree_i):\n",
    "        '''\n",
    "        :param train_data: 训练数据集\n",
    "        :param train_label: 训练集的标签，输入的标签应该是一位的，即1,2, 3......\n",
    "        :return:\n",
    "        '''\n",
    "        with self.dha_cda.as_default():\n",
    "            with tf.name_scope(self.model_name):\n",
    "\n",
    "                self.gen_model(train_data, train_label)\n",
    "                dha_loss = self.dha_loss()\n",
    "\n",
    "                with tf.name_scope('Optimizer'):\n",
    "                    learning_rate = tf.train.exponential_decay(self.lr, self.train_step, decay_steps=100,\n",
    "                                                               decay_rate=0.8)\n",
    "                    train_op = tf.train.AdamOptimizer(learning_rate).minimize(dha_loss)\n",
    "\n",
    "                for weight in self.weights:\n",
    "                    tf.add_to_collection('LRP_CDA_weights'+str(tree_i), weight)\n",
    "\n",
    "                for act in self.activations:\n",
    "                    tf.add_to_collection('LRP_CDA_activations'+str(tree_i), act)\n",
    "\n",
    "                saver = tf.train.Saver()\n",
    "                with tf.Session(graph=self.dha_cda) as sess:\n",
    "                    sess.run(tf.global_variables_initializer())\n",
    "                    # writer = tf.summary.FileWriter(\"demo_class\", sess.graph)\n",
    "                    # writer.close()\n",
    "\n",
    "                    feed_dicts = {}\n",
    "                    for i in range(len(self.split_num)):\n",
    "                        feed_dicts[self.names['V{}'.format(i)]] = self.data_space['data{}'.format(i)]\n",
    "\n",
    "                    sess.run([self.loss_cae1, self.optimizer_cae1, self.loss_cae2,self.optimizer_cae2,\n",
    "                              self.loss_cae3, self.optimizer_cae3], feed_dict=feed_dicts)\n",
    "\n",
    "                    self.loss_list = []\n",
    "                    self.R_history = []\n",
    "\n",
    "                    print('Enter train the Space........')\n",
    "                    t1_ = time.time()\n",
    "                    feed_batch_dicts = {}\n",
    "                    for j in range(self.train_step):\n",
    "                        for batch_i in range(int(np.ceil(len(train_data) / self.batch_size))):\n",
    "                            for i in range(len(self.split_num)):\n",
    "                                feed_batch_dicts[self.names['V{}'.format(i)]] = random.choices(\n",
    "                                    self.data_space['data{}'.format(i)],\n",
    "                                    k=int(self.batch_size/self.fault_num))\n",
    "                            _ = sess.run(train_op, feed_dict=feed_batch_dicts)\n",
    "                            loss = sess.run(self.loss_all, feed_dict=feed_batch_dicts)\n",
    "\n",
    "                        self.loss_list.append(loss)\n",
    "                        R = sess.run([self.names['R{}'.format(i)] for i in range(len(self.split_num))])  # 半径\n",
    "                        self.R_history.append(R)\n",
    "                        print(\" Epoch\", j, \": loss : \", loss)\n",
    "\n",
    "\n",
    "                    t2_ = time.time()\n",
    "                    print('训练时间：%.2f s'% (t2_ - t1_))\n",
    "\n",
    "                    self.R_list = []   # 半径\n",
    "                    self.circle = []    # 球心\n",
    "                    self.circle = sess.run([self.names['Cm{}'.format(i)] for i in range(self.fault_num)], feed_dict=feed_dicts)\n",
    "                    self.R_list = sess.run([self.names['R{}'.format(i)] for i in range(self.fault_num)])\n",
    "\n",
    "                    w1, w2, w3 = sess.run([self.hidden_w1, self.hidden_w2, self.hidden_w3])\n",
    "                    b1, b2, b3 = sess.run([self.hidden_b1, self.hidden_b2, self.hidden_b3])\n",
    "                    self.encoder_weight = [w1, w2, w3]\n",
    "                    self.encoder_bias = [b1, b2, b3]\n",
    "\n",
    "\n",
    "                    u_list = sess.run([self.names['U{}'.format(i)] for i in range(self.fault_num)], feed_dict=feed_dicts)\n",
    "                    DHA_pre = np.zeros(shape=[train_label.shape[0], 1])\n",
    "                    for i in range(len(self.split_num)):   # 每个类的数据\n",
    "                        var = u_list[i]\n",
    "                        dis = np.zeros(shape=[var.shape[0], len(self.split_num)])\n",
    "                        for j in range(len(self.split_num)):  # 和每个球心求距离\n",
    "\n",
    "                            center = tf.reshape(tf.tile(self.circle[j], [self.split_num[i]]),\n",
    "                                                [self.split_num[i], len(self.circle[j])])   # tf.tile 对张量进行复制\n",
    "                            dis_c = sess.run(tf.linalg.norm(var - center, axis=1))\n",
    "                            dis_c = dis_c / self.R_list[j]\n",
    "                            dis[:, j] = dis_c\n",
    "\n",
    "                        tmp_id = self.index_space['index{}'.format(i)]\n",
    "                        DHA_pre[tmp_id] = np.argmin(dis, axis=1).reshape(-1, 1)\n",
    "\n",
    "                    DHA_pre = DHA_pre.astype(np.int64)\n",
    "                    acc = tf.reduce_mean(tf.cast(tf.equal(DHA_pre, train_label), tf.float32))\n",
    "                    acc = sess.run(acc)\n",
    "\n",
    "                    # 画r和loss的变化曲线图\n",
    "                    dir1 = './DHA_CDA_jnu/'\n",
    "                    if not os.path.exists(dir1):\n",
    "                        os.mkdir(dir1)\n",
    "                    picturesdir = './DHA_CDA_jnu/pictures/'\n",
    "                    if not os.path.exists(picturesdir):\n",
    "                        os.mkdir(picturesdir)\n",
    "                    # fig1 = Visualization(train_label, DHA_pre, picturesdir)\n",
    "                    # fig1.plot_R(self.R_history, self.split_num)\n",
    "                    # fig1.plot_loss(self.loss_list)\n",
    "                    saver.save(sess, \"./DHA_CDA_jnu/model_\" + str(tree_i))  # 用于LRP\n",
    "                    sess.close()\n",
    "\n",
    "                    return DHA_pre, self.circle, self.R_list, self.encoder_weight, self.encoder_bias, loss, acc\n",
    "\n",
    "\n",
    "    # -------------------距离计算区----------------\n",
    "    # 测试数据集 在空间W中到球心的距离\n",
    "    def test(self, testdata, test_label, Cm, R, w, b, test=False):\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            hidden1 = tf.nn.sigmoid(tf.add(tf.matmul(testdata, w[0]), b[0]))\n",
    "            hidden2 = tf.nn.sigmoid(tf.add(tf.matmul(hidden1, w[1]), b[1]))\n",
    "            space_test = tf.nn.sigmoid(tf.add(tf.matmul(hidden2, w[2]), b[2]))\n",
    "\n",
    "            m = space_test.shape[0]\n",
    "            test_pre = np.zeros(shape=[testdata.shape[0], self.fault_num])\n",
    "            for i in range(len(self.split_num)):\n",
    "\n",
    "                center = tf.reshape(tf.tile(Cm[i], [m]), [m, len(Cm[i])])  # tf.tile 对张量进行复制\n",
    "                distance = sess.run(tf.linalg.norm(space_test - center, axis=1))\n",
    "                distance /= R[i]\n",
    "                test_pre[:, i] = distance\n",
    "\n",
    "            test_pre = np.argmin(test_pre, axis=1).astype(np.int64).reshape(-1, 1)\n",
    "            acc = tf.reduce_mean(tf.cast(tf.equal(test_pre, test_label), tf.float32))\n",
    "            acc = sess.run(acc)\n",
    "            print(acc)\n",
    "\n",
    "            picturesdir = './DHA_CDA_jnu/pictures/'\n",
    "\n",
    "            acc = accuracy_score(test_label, test_pre)\n",
    "            print('accuracy：', acc)\n",
    "            rec_w = recall_score(test_label, test_pre, average='macro')\n",
    "            print('recall score (macro)：', rec_w)\n",
    "            f1_w = f1_score(test_label, test_pre, average='macro')\n",
    "            print('f1_score (macro)：', f1_w)\n",
    "            precision_w = precision_score(test_label, test_pre, average='macro')\n",
    "            print('precision_score (macro)：', precision_w)\n",
    "\n",
    "            if test == True:\n",
    "                fig2 = Visualization(test_label, test_pre, picturesdir)\n",
    "                acc = fig2.plot_confusion_matrix('DHA_CDA test HAR Confusion Matrix')  # 画混淆矩阵\n",
    "                space_test = sess.run(space_test)\n",
    "                fig2.pca_2D(space_test)\n",
    "                fig2.tsne_3d(space_test)\n",
    "                return acc, test_pre, space_test\n",
    "\n",
    "                # fig2.pca_encoder_2D(space_test)\n",
    "\n",
    "            # fig2 = Visualization(test_label, test_pre, picturesdir)\n",
    "            # acc = fig2.plot_confusion_matrix('DHA_CDA test HAR Confusion Matrix')  # 画混淆矩阵\n",
    "\n",
    "            # sess.close()\n",
    "            else:\n",
    "                return acc, test_pre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DHA_SDA():\n",
    "    def __init__(self, unit1, unit2, unit3, model_name='sda_scope', train_step=10, fault_num=10,\n",
    "                 batch_size=500, lr=0.001, R_constant = 1.0, P=1.0, P_R2Cm=1.0, P_class=0.0001):\n",
    "        \"\"\"\n",
    "        :param DataFram:  数据集\n",
    "        :param train_step:训练步数\n",
    "        :param R_constant:初始化半径\n",
    "        :param lr:        优化器学习率\n",
    "        :param P:         每个球体的惩罚系数\n",
    "        :param P_R2Cm:    球体之间的惩罚系数\n",
    "        :param P_class:   加速收敛系数\n",
    "        \"\"\"\n",
    "        self.train_step = train_step\n",
    "        self.lr = lr\n",
    "        self.P = P\n",
    "        self.P_R2Cm = P_R2Cm\n",
    "        self.P_class = P_class\n",
    "        self.R_constant = R_constant\n",
    "\n",
    "        # 编码器部分\n",
    "        self.lambda1 = 1e-4  # 权重衰减项\n",
    "        self.beta = 0.002  # 稀疏惩罚项系数\n",
    "        self.rho = 0.2  # 稀疏因子\n",
    "        self.keep_prop = 0.98\n",
    "        self.batch_size = batch_size\n",
    "        self.unit1 = unit1\n",
    "        self.unit2 = unit2\n",
    "        self.unit3 = unit3\n",
    "        self.pop = np.array([600, self.unit1, self.unit2, self.unit3, 1])\n",
    "        self.model_name = model_name\n",
    "        self.dha_sda = tf.Graph()\n",
    "        self.fault_num = fault_num\n",
    "\n",
    "        self.data_space = {}\n",
    "        self.index_space = {}\n",
    "        self.activations = []\n",
    "        self.names = {}\n",
    "        self.split_num = []  # 每个类别的数据的个数\n",
    "\n",
    "        # lrp\n",
    "        self.weights = []\n",
    "        self.activations = []\n",
    "\n",
    "\n",
    "    '''\n",
    "    加入稀疏性约束，rho_hat是输入层所有神经元在隐层第j个神经元上激活值的平均，rho是稀疏因子\n",
    "    稀疏自编码器代码： \n",
    "    https://github.com/summersunshine1/datamining/blob/master/sparseencoder/sparseAutoEncoder.py\n",
    "    '''\n",
    "    # 编码器的损失函数\n",
    "    def sda_loss(self, data, outputdata, hidden, hidden_w, output_w):\n",
    "\n",
    "        error_loss = tf.reduce_mean(tf.square(outputdata - data))\n",
    "        tf.add_to_collection(\"losses\", error_loss)\n",
    "        # 参数L2正则化：在原来的损失函数基础上加上权重参数的平方和： 限制参数过多或者过大，避免模型更复杂，可以降低过拟合\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(self.lambda1)\n",
    "        regularization = regularizer(hidden_w) + regularizer(output_w)\n",
    "        tf.add_to_collection(\"losses\", regularization)\n",
    "        # 加入稀疏性约束\n",
    "        rho_hat = tf.reduce_mean(hidden, 0)\n",
    "        Sparse_cost = self.beta * (tf.reduce_sum(self.rho * tf.log(self.rho / rho_hat) + (1 - self.rho) *\n",
    "                                                 tf.log((1 - self.rho) / (1 - rho_hat))))\n",
    "        tf.add_to_collection(\"losses\", Sparse_cost)\n",
    "\n",
    "        loss_sae = tf.add_n(tf.get_collection(\"losses\"))\n",
    "        optimizer_sae = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss_sae)\n",
    "        return loss_sae, optimizer_sae\n",
    "\n",
    "\n",
    "    # 在圆心外惩罚\n",
    "    def g1n_term(self, var, center, Rm):\n",
    "        g1n = tf.linalg.norm(var - center, axis=1) - Rm  # 点到球心的距离 - 半径 = 点到球的距离\n",
    "        g1n_max = tf.clip_by_value(g1n, 0, 1e10)\n",
    "        penalty = tf.reduce_mean(g1n_max)  # if res>0, penalty = res else penalty = 0\n",
    "        return penalty\n",
    "\n",
    "    # 在圆心内惩罚\n",
    "    def g2n_term(self, var, center, Rm):\n",
    "        g2n = Rm - tf.linalg.norm(var - center, axis=1)  # 半径 - 点到球心的距离 = 点到球边缘的距离\n",
    "        g2n_max = tf.clip_by_value(g2n, 0, 1e10)\n",
    "        penalty = tf.reduce_mean(g2n_max)  # if res>0,penalty = res else penalty = 0\n",
    "        return penalty\n",
    "\n",
    "\n",
    "    # DHA分类器的损失函数\n",
    "    def dha_loss(self):\n",
    "        with self.dha_sda.as_default():\n",
    "            with tf.name_scope(self.model_name):\n",
    "                with tf.name_scope('loss_pow'):\n",
    "                    split_list = np.arange(0, len(self.split_num))\n",
    "                    for i in range(len(self.split_num)):\n",
    "                        with tf.name_scope('loss_pow{}'.format(i)):\n",
    "                            split_list_ = np.delete(split_list, i)\n",
    "                            if len(split_list_) == 1:\n",
    "                                U_ = self.names['U{}'.format(split_list_[0])]\n",
    "                            else:\n",
    "                                U_ = tf.concat([self.names['U{}'.format(j)] for j in split_list_], 0)\n",
    "\n",
    "                            g1n1 = self.g1n_term(self.names['U{}'.format(i)],  # layer：weight * v + bias\n",
    "                                                 self.names['Cm{}'.format(i)],  # 球心\n",
    "                                                 self.names['R{}'.format(i)])  # 半径\n",
    "                            g2n1 = self.g2n_term(U_,\n",
    "                                                 self.names['Cm{}'.format(i)],\n",
    "                                                 self.names['R{}'.format(i)])\n",
    "\n",
    "                            Rn = tf.where(tf.greater(np.float64(0), self.names['R{}'.format(i)]),\n",
    "                                          self.names['R{}'.format(i)], 0)\n",
    "                            self.names['loss{}_pow'.format(i)] = tf.pow(g1n1, 2) + tf.pow(g2n1, 2) + tf.pow(Rn, 2)\n",
    "\n",
    "                    loss_pow = 0\n",
    "                    for i in range(len(self.split_num)):\n",
    "                        loss_pow = loss_pow + self.names['loss{}_pow'.format(i)]\n",
    "\n",
    "\n",
    "                with tf.name_scope('lossR2Cm'):\n",
    "                    combine = list(itertools.combinations(np.arange(0, len(self.split_num)).tolist(), 2))\n",
    "                    loss_R2Cm = 0\n",
    "                    for i in combine:\n",
    "                        with tf.name_scope('lossR2Cm{}{}'.format(i[0], i[1])):\n",
    "                            Cm_normal = (self.names['R{}'.format(i[0])] + self.names['R{}'.format(i[1])]) \\\n",
    "                                        - tf.linalg.norm(self.names['Cm{}'.format(i[0])] - self.names['Cm{}'.format(i[1])])\n",
    "                            loss_R2Cm = loss_R2Cm + tf.where(tf.greater(Cm_normal, 0), Cm_normal, 0)\n",
    "\n",
    "\n",
    "                with tf.name_scope('loss_class'):\n",
    "                    loss_class = 0.0\n",
    "                    for i in range(len(self.split_num)):\n",
    "                        loss_class = loss_class + tf.linalg.norm(self.names['U{}'.format(i)] - self.names['Cm{}'.format(i)])\n",
    "\n",
    "                with tf.name_scope('loss_all'):\n",
    "                    self.loss_all = self.P * (loss_pow) + self.P_R2Cm * loss_R2Cm + self.P_class * loss_class\n",
    "\n",
    "            return self.loss_all\n",
    "\n",
    "\n",
    "    def gen_model(self, train_data, train_label):\n",
    "        with self.dha_sda.as_default():\n",
    "            with tf.name_scope(self.model_name):\n",
    "\n",
    "                for class_i in range(self.fault_num):\n",
    "                    i_d = np.argwhere(train_label == class_i)\n",
    "                    i_d = i_d[:, 0]\n",
    "                    self.split_num.append(len(i_d))\n",
    "                    self.data_space['data{}'.format(class_i)] = train_data[i_d]\n",
    "                    self.index_space['index{}'.format(class_i)] = i_d  # 二维\n",
    "\n",
    "\n",
    "                with tf.name_scope('InitVariable'):\n",
    "                    for i in range(len(self.split_num)):\n",
    "                        with tf.name_scope('V{}'.format(i)):\n",
    "                            self.names['V{}'.format(i)] = tf.placeholder(\n",
    "                                tf.float64, shape=[None, train_data.shape[1]], name='Input{}'.format(i))\n",
    "\n",
    "\n",
    "                    with tf.name_scope('weight'):\n",
    "                        self.hidden_w1 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[0], self.pop[1]]),\n",
    "                                                        trainable=True), tf.float64, name='hidden_w1')  # float32\n",
    "                        output_w1 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[1], self.pop[0]]),\n",
    "                                                        trainable=True), tf.float64, name='output_w_1')\n",
    "                        self.hidden_w2 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[1], self.pop[2]]),\n",
    "                                                        trainable=True), tf.float64, name='hidden_w2')  # float32\n",
    "                        output_w2 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[2], self.pop[1]]),\n",
    "                                                        trainable=True), tf.float64, name='output_w_2')\n",
    "                        self.hidden_w3 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[2], self.pop[3]]),\n",
    "                                                        trainable=True), tf.float64, name='hidden_w3')  # float32\n",
    "                        output_w3 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[3], self.pop[2]]),\n",
    "                                                     trainable=True), tf.float64, name='out_w_3')\n",
    "\n",
    "                        self.weights = [self.hidden_w1, self.hidden_w2, self.hidden_w3]\n",
    "\n",
    "\n",
    "                    with tf.name_scope('bias'):\n",
    "                        self.hidden_b1 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[4], self.pop[1]]),\n",
    "                                                        trainable=True), tf.float64, name='hidden_b1')\n",
    "                        output_b1 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[4], self.pop[0]]),\n",
    "                                                        trainable=True), tf.float64, name='output_b_1')\n",
    "                        self.hidden_b2 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[4], self.pop[2]]),\n",
    "                                                        trainable=True), tf.float64, name='hidden_b2')\n",
    "                        output_b2 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[4], self.pop[1]]),\n",
    "                                                        trainable=True), tf.float64, name='output_b_2')\n",
    "                        self.hidden_b3 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[4], self.pop[3]]),\n",
    "                                                        trainable=True), tf.float64, name='hidden_b3')\n",
    "                        output_b3 = tf.cast(tf.Variable(initial_value=tf.random_normal(shape=[self.pop[4], self.pop[2]]),\n",
    "                                                     trainable=True), tf.float64, name='out_b_3')\n",
    "\n",
    "\n",
    "                    # R_constant:初始化半径\n",
    "                    with tf.name_scope('R'):\n",
    "                        for i in range(len(self.split_num)):\n",
    "                            self.names['R{}'.format(i)] = tf.Variable(initial_value=self.R_constant,\n",
    "                                                                 dtype=tf.float64, name='R{}'.format(i), trainable=True)\n",
    "\n",
    "\n",
    "                with tf.name_scope('encoder_layer'):\n",
    "\n",
    "                    for i in range(len(self.split_num)):\n",
    "                        hidden1 = tf.nn.sigmoid(tf.add(tf.matmul(self.names['V{}'.format(i)], self.hidden_w1), self.hidden_b1))\n",
    "                        output_ae1 = tf.nn.sigmoid(tf.add(tf.matmul(hidden1, output_w1), output_b1))\n",
    "                        hidden1 = tf.nn.dropout(hidden1, keep_prob=self.keep_prop)\n",
    "                        hidden2 = tf.nn.sigmoid(tf.add(tf.matmul(hidden1, self.hidden_w2), self.hidden_b2))\n",
    "                        output_ae2 = tf.nn.sigmoid(tf.add(tf.matmul(hidden2, output_w2), output_b2))\n",
    "                        hidden2 = tf.nn.dropout(hidden2, keep_prob=self.keep_prop)\n",
    "                        hidden3 = tf.nn.sigmoid(tf.add(tf.matmul(hidden2, self.hidden_w3), self.hidden_b3))\n",
    "                        out_ae3 = tf.nn.sigmoid(tf.add(tf.matmul(hidden3, output_w3), output_b3))\n",
    "                        self.names['U{}'.format(i)] = tf.nn.dropout(hidden3, keep_prob=self.keep_prop)\n",
    "\n",
    "\n",
    "                        # 逐层训练\n",
    "                        self.loss_ae1, self.optimizer_ae1 = self.sda_loss(\n",
    "                            self.names['V{}'.format(i)], output_ae1, hidden1, self.hidden_w1, output_w1)\n",
    "\n",
    "                        self.loss_ae2, self.optimizer_ae2 = self.sda_loss(\n",
    "                                hidden1, output_ae2, hidden2, self.hidden_w2, output_w2)\n",
    "\n",
    "                        self.loss_ae3, self.optimizer_ae3 = self.sda_loss(\n",
    "                            hidden2, out_ae3, self.names['U{}'.format(i)], self.hidden_w3, output_w3)\n",
    "\n",
    "                    self.activations = [self.names['V{}'.format(i)], hidden1, hidden2, self.names['U{}'.format(i)]]\n",
    "\n",
    "\n",
    "                with tf.name_scope('circle'):\n",
    "                    for i in range(len(self.split_num)):\n",
    "                        self.names['Cm{}'.format(i)] = tf.reduce_mean(self.names['U{}'.format(i)], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, train_data, train_label, tree_i):\n",
    "        with self.dha_sda.as_default():\n",
    "            with tf.name_scope(self.model_name):\n",
    "\n",
    "                self.gen_model(train_data, train_label)\n",
    "                cl_loss = self.dha_loss()\n",
    "\n",
    "                with tf.name_scope('Optimizer'):\n",
    "                    learning_rate = tf.train.exponential_decay(self.lr, self.train_step, decay_steps=100,\n",
    "                                                               decay_rate=0.8)\n",
    "                    train_op = tf.train.AdamOptimizer(learning_rate).minimize(cl_loss)\n",
    "\n",
    "                for weight in self.weights:\n",
    "                    tf.add_to_collection('LRP_SDA_weights'+str(tree_i), weight)\n",
    "\n",
    "                for act in self.activations:\n",
    "                    tf.add_to_collection('LRP_SDA_activations'+str(tree_i), act)\n",
    "                # we = tf.get_collection('LRP_SDA_weights')\n",
    "                # print(we)\n",
    "                # ac = tf.get_collection('LRP_SDA_activations')\n",
    "                # print(ac)\n",
    "\n",
    "                saver = tf.train.Saver()\n",
    "                with tf.Session(graph=self.dha_sda) as sess:\n",
    "                    sess.run(tf.global_variables_initializer())\n",
    "                    # writer = tf.summary.FileWriter(\"demo_class\", sess.graph)\n",
    "                    # writer.close()\n",
    "\n",
    "                    feed_dicts = {}\n",
    "                    for i in range(len(self.split_num)):\n",
    "                        feed_dicts[self.names['V{}'.format(i)]] = self.data_space['data{}'.format(i)]\n",
    "\n",
    "                    sess.run([self.loss_ae1, self.optimizer_ae1, self.loss_ae2, self.optimizer_ae2,\n",
    "                              self.loss_ae3, self.optimizer_ae3], feed_dict=feed_dicts)\n",
    "\n",
    "                    self.loss_list = []\n",
    "                    self.R_history = []\n",
    "\n",
    "                    print('Enter train the Space........')\n",
    "                    t1_ = time.time()\n",
    "                    feed_batch_dicts = {}\n",
    "                    for j in range(self.train_step):\n",
    "                        for batch_i in range(int(np.ceil(len(train_data) / self.batch_size))):\n",
    "                            for i in range(len(self.split_num)):\n",
    "                                feed_batch_dicts[self.names['V{}'.format(i)]] = random.choices(self.data_space['data{}'.format(i)],\n",
    "                                                                                    k=int(self.batch_size/self.fault_num))\n",
    "                            _ = sess.run(train_op, feed_dict=feed_batch_dicts)\n",
    "                            loss = sess.run(self.loss_all, feed_dict=feed_batch_dicts)\n",
    "\n",
    "                        self.loss_list.append(loss)\n",
    "                        R = sess.run([self.names['R{}'.format(i)] for i in range(len(self.split_num))])  # 半径\n",
    "                        self.R_history.append(R)\n",
    "                        print(\" Epoch\", j, \": loss : \", loss)\n",
    "\n",
    "\n",
    "\n",
    "                    t2_ = time.time()\n",
    "                    print('训练时间：%.2f s'% (t2_ - t1_))\n",
    "\n",
    "                    self.R_list = []\n",
    "                    self.circle = []\n",
    "                    self.circle = sess.run([self.names['Cm{}'.format(i)] for i in range(self.fault_num)], feed_dict=feed_dicts)\n",
    "                    self.R_list = sess.run([self.names['R{}'.format(i)] for i in range(self.fault_num)])\n",
    "\n",
    "                    w1, w2, w3 = sess.run([self.hidden_w1, self.hidden_w2, self.hidden_w3])\n",
    "                    b1, b2, b3 = sess.run([self.hidden_b1, self.hidden_b2, self.hidden_b3])\n",
    "                    self.encoder_weight = [w1, w2, w3]\n",
    "                    self.encoder_bias = [b1, b2, b3]\n",
    "\n",
    "\n",
    "                    u_list = sess.run([self.names['U{}'.format(i)] for i in range(self.fault_num)], feed_dict=feed_dicts)\n",
    "                    DHA_pre = np.zeros(shape=[train_label.shape[0], 1])\n",
    "                    for i in range(len(self.split_num)): # 每个类的数据\n",
    "                        var = u_list[i]\n",
    "                        dis = np.zeros(shape=[var.shape[0], len(self.split_num)])\n",
    "                        for j in range(len(self.split_num)):  # 和每个球心求距离\n",
    "\n",
    "                            center = tf.reshape(tf.tile(self.circle[j], [self.split_num[i]]),\n",
    "                                                [self.split_num[i], len(self.circle[j])])   # tf.tile 对张量进行复制\n",
    "                            dis_c = sess.run(tf.linalg.norm(var - center, axis=1))\n",
    "                            dis_c = dis_c / self.R_list[j]\n",
    "                            dis[:, j] = dis_c\n",
    "\n",
    "                        tmp_id = self.index_space['index{}'.format(i)]\n",
    "                        DHA_pre[tmp_id] = np.argmin(dis, axis=1).reshape(-1, 1)\n",
    "\n",
    "                    DHA_pre = DHA_pre.astype(np.int64)\n",
    "                    acc = tf.reduce_mean(tf.cast(tf.equal(DHA_pre, train_label), tf.float32))\n",
    "                    acc = sess.run(acc)\n",
    "\n",
    "                    # 画r和loss的变化曲线图\n",
    "                    dir1 = './DHA_SDA_jnu/'\n",
    "                    if not os.path.exists(dir1):\n",
    "                        os.mkdir(dir1)\n",
    "                    picturesdir = './DHA_SDA_jnu/pictures/'\n",
    "                    if not os.path.exists(picturesdir):\n",
    "                        os.mkdir(picturesdir)\n",
    "                    # fig1 = Visualization(train_label, DHA_pre, picturesdir)\n",
    "                    # fig1.plot_R(self.R_history, self.split_num)\n",
    "                    # fig1.plot_loss(self.loss_list)\n",
    "\n",
    "                    saver.save(sess, \"./DHA_SDA_jnu/model_\" + str(tree_i))\n",
    "                    # sess.close()\n",
    "                    return DHA_pre, self.circle, self.R_list, self.encoder_weight, self.encoder_bias, loss, acc\n",
    "\n",
    "\n",
    "    # -------------------距离计算区----------------\n",
    "    # 测试数据集 在空间W中到球心的距离\n",
    "    def test(self, testdata, label, Cm, R, w, b, test=False):\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            hidden1 = tf.nn.sigmoid(tf.add(tf.matmul(testdata, w[0]), b[0]))\n",
    "            hidden2 = tf.nn.sigmoid(tf.add(tf.matmul(hidden1, w[1]), b[1]))\n",
    "            space_test = tf.nn.sigmoid(tf.add(tf.matmul(hidden2, w[2]), b[2]))\n",
    "\n",
    "            m = space_test.shape[0]\n",
    "            test_pre = np.zeros(shape=[testdata.shape[0], self.fault_num])\n",
    "            for i in range(len(self.split_num)):\n",
    "\n",
    "                center = tf.reshape(tf.tile(Cm[i], [m]), [m, len(Cm[i])])  # tf.tile 对张量进行复制\n",
    "                distance = sess.run(tf.linalg.norm(space_test - center, axis=1))\n",
    "                distance /= R[i]\n",
    "                test_pre[:, i] = distance\n",
    "\n",
    "            test_pre = np.argmin(test_pre, axis=1).astype(np.int64).reshape(-1, 1)\n",
    "            acc = tf.reduce_mean(tf.cast(tf.equal(test_pre, label), tf.float32))\n",
    "            acc = sess.run(acc)\n",
    "\n",
    "            picturesdir = './DHA_SDA_jnu/pictures/'\n",
    "            if not os.path.exists(picturesdir):\n",
    "                os.mkdir(picturesdir)\n",
    "\n",
    "            acc = accuracy_score(label, test_pre)\n",
    "            print('accuracy：', acc)\n",
    "            rec_w = recall_score(label, test_pre, average='macro')\n",
    "            print('recall score (macro)：', rec_w)\n",
    "            f1_w = f1_score(label, test_pre, average='macro')\n",
    "            print('f1_score (macro)：', f1_w)\n",
    "            precision_w = precision_score(label, test_pre, average='macro',zero_division=1)\n",
    "            print('precision_score (macro)：', precision_w)\n",
    "\n",
    "            if test==True:\n",
    "                evaluation = Visualization(label, test_pre, picturesdir)\n",
    "                space_test = sess.run(space_test)\n",
    "                # evaluation.pca_2D(space_test)\n",
    "                # evaluation.tsne_3d(space_test)\n",
    "                acc = evaluation.plot_confusion_matrix('DHA_SDA test HAR Confusion Matrix')  # 画混淆矩阵\n",
    "                return acc, test_pre, space_test\n",
    "\n",
    "            # evaluation = Visualization(label, test_pre, picturesdir)\n",
    "            #\n",
    "            # acc = evaluation.plot_confusion_matrix('DHA_SDA test HAR Confusion Matrix')  # 画混淆矩阵\n",
    "\n",
    "            # evaluation.plot_roc('DHA_SDA roc')\n",
    "\n",
    "            # sess.close()\n",
    "            else:\n",
    "                return acc,  test_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import heapq\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_fun(X, data, data_label, encoder, train_step, fault_num):  # 适应函数\n",
    "\n",
    "    if encoder == 'sda_para':\n",
    "        sda = DHA_SDA(X[0], X[1], X[2], lr=X[3], train_step=train_step, fault_num=fault_num)\n",
    "        _, _, _, _, _, cost, accuracy = sda.fit(data, data_label, -2)\n",
    "    elif encoder == 'cda_para':\n",
    "        cda = DHA_CDA(X[0], X[1], X[2], lr=X[3], train_step=train_step, fault_num=fault_num)\n",
    "        _, _, _, _, _, cost, accuracy = cda.fit(data, data_label, -2)\n",
    "    elif encoder == 'dda_para':\n",
    "        dda = DHA_DDA(X[0], X[1], X[2], lr=X[3], train_step=train_step, fault_num=fault_num)\n",
    "        _, _, _, _, _, cost, accuracy = dda.fit(data, data_label, -2)\n",
    "\n",
    "    return cost, accuracy\n",
    "\n",
    "\n",
    "class Particle:  # 粒子\n",
    "    # 初始化\n",
    "    def __init__(self, para_max, para_min, dim, data, data_label, encoder_para, fault_num, encoder_iternum=20):\n",
    "\n",
    "         # 粒子的位置\n",
    "        self.fitnessValue_cost = None\n",
    "        self.pos = [(np.random.rand() * (para_max[i] - para_min[i]))+ para_min[i]\n",
    "                  for i in range(dim)]\n",
    "        for i in range(len(self.pos)):\n",
    "            if i != len(self.pos)-1:\n",
    "                tmp_x = math.modf(self.pos[i])  # 分别取出整数部分和小数部分  输出tuple（1，2）2为整数部分，1为小数部分\n",
    "                x_1 = int(tmp_x[1])  # 把整数部分转换为int\n",
    "                if tmp_x[0] >= 0.5:\n",
    "                    self.pos[i] = x_1 + 1\n",
    "                else:\n",
    "                    self.pos[i] = x_1\n",
    "\n",
    "        self.bestPos = [0.0 for i in range(dim)]  # 粒子最好的位置\n",
    "        cost, _ = fit_fun(self.pos, data, data_label, encoder_para, encoder_iternum, fault_num)  # 适应度函数值\n",
    "        self.fitnessValue_cost = cost\n",
    "\n",
    "    def set_pos(self, value):\n",
    "        self.pos = value\n",
    "\n",
    "    def get_pos(self):\n",
    "        return self.pos\n",
    "\n",
    "    def set_best_pos(self, value):\n",
    "        self.bestPos = value\n",
    "\n",
    "    def get_best_pos(self):\n",
    "        return self.bestPos\n",
    "\n",
    "    def set_fitness_value(self, value1):\n",
    "        self.fitnessValue_cost = value1\n",
    "\n",
    "    def get_cost_value(self):\n",
    "        return self.fitnessValue_cost\n",
    "\n",
    "\n",
    "class AOA:\n",
    "    def __init__(self, size, aoa_iter_num, data, data_label, sda_para=False, cda_para=False, dda_para=False,\n",
    "                 best_fitness_cost=float('Inf'),  dim=4, encoder_iternum=10, fault_num=10):\n",
    "\n",
    "        self.dim = dim  # 粒子的维度\n",
    "        self.size = size  # 粒子个数\n",
    "        self.iter_num = aoa_iter_num  # aoa迭代次数\n",
    "        self.encoder_iternum = encoder_iternum\n",
    "        self.data = data\n",
    "        self.data_label = data_label\n",
    "        self.fault_num = fault_num\n",
    "        self.u = 0.5\n",
    "        self.alpha = 5\n",
    "        self.MOA_min = 0.2\n",
    "        self.MOA_max = 0.9\n",
    "        self.best_fitness_cost = best_fitness_cost\n",
    "        self.fitness_cost_list = []  # 每次迭代最优适应值\n",
    "        self.position_list = []\n",
    "        self.best_position = [0.0 for i in range(dim)]  # 种群最优位置\n",
    "\n",
    "\n",
    "        # self.para_max = [447, 334, 250, 0.01]\n",
    "        # self.para_min = [120, 75, 50, 0.002]\n",
    "        self.para_max = []\n",
    "        self.para_min = []\n",
    "        input_unit = self.data.shape[1]\n",
    "        out_unit = 3 * self.fault_num\n",
    "        r = np.power(input_unit/out_unit, 1/3)\n",
    "        min_unit1 = out_unit * np.power(r, 2)\n",
    "        min_unit2 = out_unit * r\n",
    "        self.para_min = [int(min_unit1), int(min_unit2), int(out_unit), 0.002]\n",
    "        for i in range(self.dim-1):\n",
    "            input_unit = np.sqrt(0.55 * np.power(input_unit, 2) + 3.31 * input_unit + 0.35) + 0.51\n",
    "            self.para_max.append(int(input_unit))\n",
    "        self.para_max.append(0.09)\n",
    "\n",
    "\n",
    "        if sda_para == True:\n",
    "            self.encoder_para = 'sda_para'\n",
    "        elif cda_para == True:\n",
    "            self.encoder_para = 'cda_para'\n",
    "        elif dda_para == True:\n",
    "            self.encoder_para = 'dda_para'\n",
    "\n",
    "\n",
    "        # 初始化粒子\n",
    "        self.Particle_list = []\n",
    "        for i in range(self.size):\n",
    "            p = Particle(self.para_max, self.para_min, self.dim, self.data,self.data_label,\n",
    "                                       self.encoder_para, fault_num =self.fault_num ,encoder_iternum=self.encoder_iternum)\n",
    "            self.position_list.append(p.pos)\n",
    "            self.Particle_list.append(p)\n",
    "\n",
    "        self.Particle_list.sort(key=lambda x:  x.fitnessValue_cost)\n",
    "        self.best_position = copy.deepcopy(self.Particle_list[0].pos)\n",
    "        self.best_fitness_cost = self.Particle_list[0].fitnessValue_cost\n",
    "\n",
    "\n",
    "    def set_bestFitnessValue(self, value1):\n",
    "        self.best_fitness_cost = value1\n",
    "\n",
    "    def get_bestCostValue(self):\n",
    "        return self.best_fitness_cost\n",
    "\n",
    "    def set_bestPosition(self, value):\n",
    "        self.best_position = value\n",
    "\n",
    "    def get_bestPosition(self):\n",
    "        return self.best_position\n",
    "\n",
    "\n",
    "    # 更新位置\n",
    "    def update_pos(self, part, C_iter):\n",
    "\n",
    "        # 数学优化器概率（mop），此处MOP表示当前迭代时的函数值，α是一个敏感参数，定义了迭代过程中的开发精度，根据本文的实验，该精度固定为5。\n",
    "        MOP = 1 - ((C_iter) ** (1 / self.alpha) / (self.iter_num) ** (1 / self.alpha))\n",
    "        # 数学优化器加速（Math Optimizer Accelerated function）函数：MOA，\n",
    "        # 此处MOA表示当前迭代的函数值，MOA_min，MOA_max表示加速函数的最小值和最大值， C_iter表示当前是第几次迭代\n",
    "        MOA = self.MOA_min + C_iter * ((self.MOA_max - self.MOA_min) / self.iter_num)\n",
    "\n",
    "        new_pos = [0] * self.dim\n",
    "        best_pos = self.get_bestPosition()\n",
    "\n",
    "        for i in range(self.dim):\n",
    "            # （r1、r2和r3）生成[0,1]之间的随机值\n",
    "            r1 = np.random.rand()\n",
    "            r2 = np.random.rand()\n",
    "            r3 = np.random.rand()\n",
    "\n",
    "            UB = self.para_max[i]\n",
    "            LB = self.para_min[i]\n",
    "\n",
    "            '''\n",
    "            AOA的探索算子在几个区域上随机探索搜索区域，并基于两种主要搜索策略（除法（Division）搜索策略和乘法搜索策略）寻找更好的解决方案，\n",
    "            这两种策略在等式（3）中建模。\n",
    "            对于r1>MOA（r1是一个随机数）的条件，该搜索阶段由数学优化器加速（MOA）函数进行调节。\n",
    "            在该阶段（等式（3）中的第一条规则）中，第一个运算符（Division）的条件为r2<0。5，其他运算符（Multiplication）将被忽略，直到该操作员完成其当前任务。\n",
    "            否则，第二个运算符（Multiplication）将参与执行当前任务，而不是Division（r2是一个随机数）。\n",
    "            注意，元素考虑了一个随机比例系数，以产生更多的多样化过程并探索搜索空间的不同区域。\n",
    "            '''\n",
    "            # 位置更新方程\n",
    "            if r1 > MOA:  # r1是一个随机数\n",
    "                # Exploration phase 勘探阶段\n",
    "                if r2 > 0.5:  # < ?\n",
    "                    # Division， 应用除法数学运算符\n",
    "                    new_pos[i] = best_pos[i] / (MOP + math.e) * ((UB - LB) * self.u + LB)\n",
    "                else:\n",
    "                    # Multiplication\n",
    "                    new_pos[i] = best_pos[i] * MOP * ((UB - LB) * self.u + LB)\n",
    "\n",
    "            else:\n",
    "                # Exploitation phase\n",
    "                if r3 > 0.5:  # < ?\n",
    "                    # Subtraction\n",
    "                    new_pos[i] = best_pos[i] - MOP * ((UB - LB) * self.u + LB)\n",
    "                else:\n",
    "                    # Addiction\n",
    "                    new_pos[i] = best_pos[i] + MOP * ((UB - LB) * self.u + LB)\n",
    "\n",
    "            if new_pos[i] < LB:\n",
    "                new_pos[i] = LB  # 下限\n",
    "            elif new_pos[i] > UB:\n",
    "                new_pos[i] = UB  # 限制解的范围，上限\n",
    "\n",
    "            # 单元数应该为int型，学习率可以是小数\n",
    "            if 0 <= i < self.dim-1:\n",
    "                if isinstance(new_pos[i], float):\n",
    "                    p = math.modf(new_pos[i])  # 分别取出整数部分和小数部分  输出tuple（1，2）2为整数部分，1为小数部分\n",
    "                    p_1 = int(p[1])  # 把整数部分转换为int\n",
    "                    if p[0] >= 0.5:\n",
    "                        new_pos[i] = p_1 + 1\n",
    "                    else:\n",
    "                        new_pos[i] = p_1\n",
    "\n",
    "            # 限制范围\n",
    "            if new_pos[i] < LB:\n",
    "                new_pos[i] = LB\n",
    "            elif new_pos[i] > UB:\n",
    "                new_pos[i] = UB\n",
    "\n",
    "        part.set_pos(new_pos)\n",
    "        self.position_list.append(new_pos)\n",
    "        cost, _ = fit_fun(part.get_pos(), self.data, self.data_label, self.encoder_para, train_step=self.encoder_iternum)\n",
    "        if cost < part.get_cost_value():\n",
    "            part.set_fitness_value(cost)\n",
    "            part.set_best_pos(part.get_pos())  # 局部最优值\n",
    "\n",
    "        if cost < self.get_bestCostValue():  # 全局最优值\n",
    "            self.set_bestFitnessValue(cost)\n",
    "            self.set_bestPosition(part.get_pos())\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "\n",
    "        for i in range(self.iter_num):\n",
    "            # 优化\n",
    "            for part in self.Particle_list:\n",
    "                self.update_pos(part, i)  # 更新位置\n",
    "\n",
    "            costvalue = self.get_bestCostValue()\n",
    "            self.fitness_cost_list.append(costvalue)  # 每次迭代完把当前的最优适应度存到列表\n",
    "            if costvalue <= 1.0:\n",
    "                break\n",
    "\n",
    "        fina_pos = self.get_bestPosition()\n",
    "        dir1 = './DHA_Bagging/AOA_out_para/'\n",
    "        if not os.path.exists(dir1):\n",
    "            os.mkdir(dir1)\n",
    "        f = open('./DHA_Bagging/AOA_out_para/' + self.encoder_para + '.txt', 'w')\n",
    "        print(fina_pos, file=f)\n",
    "        f.close()\n",
    "\n",
    "        print(\"PSO最优位置:\" + str(self.get_bestPosition()))\n",
    "        print(\"PSO cost最优解:\" + str(self.fitness_cost_list))\n",
    "        self.plot(self.fitness_cost_list, self.iter_num)\n",
    "        self.plot_pos(self.position_list)\n",
    "\n",
    "        return self.get_bestPosition()\n",
    "\n",
    "    def plot(self, results, num):\n",
    "        '''画图\n",
    "        '''\n",
    "        X = []\n",
    "        Y = []\n",
    "        for i in range(num):\n",
    "            X.append(i + 1)\n",
    "            Y.append(results[i])\n",
    "        plt.plot(X, Y)\n",
    "        plt.xlabel('Number of iteration', size=10)\n",
    "        plt.ylabel('Value of cost', size=10)\n",
    "        plt.title('encoder parameter optimization')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def plot_pos(self, pos_list):\n",
    "        pos_list = np.array(pos_list)\n",
    "        pca = PCA(n_components=2)\n",
    "        pca_pos = pca.fit_transform(pos_list)\n",
    "\n",
    "        plt.figure()\n",
    "        for i in range(len(pos_list)):\n",
    "            plt.scatter(pca_pos[i, 0], pca_pos[i, 1], c=i//self.size, marker='o')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bagging_encoder(object):\n",
    "    # __slots__ = [\"forest\", \"para_forest\", \"n_estimators\", \"subsamples\", \"subfeatures\", 'probablity_train',\n",
    "    # 'train_step', 'aoa_size', 'aoa_iter_num', 'fault_num']\n",
    "    def __init__(self, n_estimators, subsamples, subfeatures, train_step,\n",
    "                 aoa_size=4, aoa_iter_num=20, fault_num=10):\n",
    "        '''\n",
    "        :param n_estimators: 分类器的总数量\n",
    "        :param subsamples: 子采样个数\n",
    "        :param subfeatures: 随机选取特征个数\n",
    "        :param train_step: 分类器的迭代次数\n",
    "        :param pso_size: aoa的粒子数目\n",
    "        :param pso_iter_num: aoa训练时的迭代次数\n",
    "        '''\n",
    "\n",
    "        self.n_estimators = n_estimators\n",
    "        self.subsamples = subsamples\n",
    "        self.subfeatures = subfeatures\n",
    "        self.fault_num = fault_num\n",
    "        self.forest = list()\n",
    "        self.probablity_train = np.ones(shape=(self.n_estimators, self.fault_num, self.fault_num))  # 训练集条件概率\n",
    "        self.aoa_size = aoa_size  # 粒子数目\n",
    "        self.aoa_iter_num = aoa_iter_num   # 迭代次数\n",
    "        self.para_forest = []\n",
    "        self.train_step = train_step\n",
    "\n",
    "    def __chooseSubset(self, X, y):\n",
    "        '''\n",
    "        :param X: 输入训练特征集合\n",
    "        :param y: 输入标签值集合\n",
    "        :return: 训练子集\n",
    "        '''\n",
    "        N, cols = X.shape\n",
    "        # 随机选取特征 , random.sample从list（即range(0, cols)）中随机选取self.subfeatures个元素，作为一个列表\n",
    "        # select_cols = list(random.sample(range(0, cols), self.subfeatures))\n",
    "        # 随机选取训练子集， np.random.permutation(N)对0-N之间的序列随机排序\n",
    "        select_rows = list(np.random.permutation(N)[:self.subsamples])\n",
    "        # 获取训练子集\n",
    "        X_subset = X[select_rows, :]\n",
    "        # X_subset = X_subset[:, select_cols]\n",
    "        y_subset = y[select_rows]\n",
    "        return X_subset, y_subset\n",
    "\n",
    "    def select_encoder(self, train_data, train_label, n_folds=3):\n",
    "\n",
    "        coeff = 2\n",
    "        kf = KFold(n_splits=n_folds)\n",
    "        weight_ratio = [1 / 3, 1 / 3, 1 / 3]\n",
    "\n",
    "        for ii, (train_index, verify_index) in enumerate(kf.split(train_data)):\n",
    "            print('第', ii+1, '折交叉验证')\n",
    "\n",
    "            x_train, y_train = train_data[train_index], train_label[train_index]\n",
    "            verify_x, verify_y = train_data[verify_index], train_label[verify_index]\n",
    "\n",
    "            dha_sda = DHA_SDA(self.pos['sda'][0], self.pos['sda'][1], self.pos['sda'][2], lr=self.pos['sda'][3],\n",
    "                              train_step=self.train_step, fault_num=self.fault_num)\n",
    "            dha_dda = DHA_DDA(self.pos['dda'][0], self.pos['dda'][1], self.pos['dda'][2], lr=self.pos['dda'][3],\n",
    "                              train_step=self.train_step, fault_num=self.fault_num)\n",
    "            dha_cda = DHA_CDA(self.pos['cda'][0], self.pos['cda'][1], self.pos['cda'][2], lr=self.pos['cda'][3],\n",
    "                              train_step=self.train_step, fault_num=self.fault_num)\n",
    "\n",
    "            sda_pre, sda_circle, sda_R, sda_w, sda_b, _, _ = dha_sda.fit(x_train, y_train, -1)\n",
    "            acc_sda, _ = dha_sda.test(verify_x, verify_y, sda_circle, sda_R, sda_w, sda_b)\n",
    "\n",
    "            cda_pre, cda_circle, cda_R, cda_w, cda_b, _, _ = dha_cda.fit(x_train, y_train, -1)\n",
    "            acc_cda, _ = dha_cda.test(verify_x, verify_y, cda_circle, cda_R, cda_w, cda_b)\n",
    "\n",
    "            dda_pre, dda_circle, dda_R, dda_w, dda_b, _, _ = dha_dda.fit(x_train, y_train, -1)\n",
    "            acc_dda, _ = dha_dda.test(verify_x, verify_y, dda_circle, dda_R, dda_w, dda_b)\n",
    "\n",
    "            acc_verify = [acc_sda, acc_dda, acc_cda]\n",
    "            alpha = [np.exp(coeff * h) for h in acc_verify]\n",
    "\n",
    "            sum_w_1 = alpha[0] * weight_ratio[0]\n",
    "            sum_w_2 = alpha[1] * weight_ratio[1]\n",
    "            sum_w_3 = alpha[2] * weight_ratio[2]\n",
    "            sum = sum_w_1 + sum_w_2 + sum_w_3\n",
    "\n",
    "            for j in range(3):\n",
    "                weight_ratio[j] = (weight_ratio[j] * alpha[j]) / sum\n",
    "            print(weight_ratio)\n",
    "\n",
    "        return weight_ratio\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        :param X: 输入训练特征集合\n",
    "        :param y: 输入标签值集合，一维\n",
    "        :return: 随机森林回归器\n",
    "        '''\n",
    "        start_train = time.clock()\n",
    "        self.pos = {}\n",
    "\n",
    "        # aoa调参\n",
    "        aoa_sda = AOA(self.aoa_size, self.aoa_iter_num, X, y, sda_para=True,\n",
    "                      encoder_iternum=self.train_step, fault_num=self.fault_num)\n",
    "        best_pos_sda = aoa_sda.update()\n",
    "        print(\"DHA_SDA最优位置:\" + str(best_pos_sda))\n",
    "        self.pos['sda'] = best_pos_sda\n",
    "\n",
    "        aoa_dda = AOA(self.aoa_size, self.aoa_iter_num, X, y, dda_para=True,\n",
    "                      encoder_iternum=self.train_step, fault_num=self.fault_num)\n",
    "        best_pos_dda = aoa_dda.update()\n",
    "        print(\"DHA_DDA最优位置:\" + str(best_pos_dda))\n",
    "        self.pos['dda'] = best_pos_dda\n",
    "\n",
    "        aoa_cda = AOA(self.aoa_size, self.aoa_iter_num, X, y, cda_para=True,\n",
    "                      encoder_iternum=self.train_step, fault_num=self.fault_num)\n",
    "        best_pos_cda = aoa_cda.update()\n",
    "        print(\"DHA_CDA最优位置:\" + str(best_pos_cda))\n",
    "        self.pos['cda'] = best_pos_cda\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            # 获取构建单棵树的训练子集\n",
    "            X_subset, y_subset = self.__chooseSubset(X, y) # y_subset：一维\n",
    "            # 训练单棵树\n",
    "            print('选择第', i+1, '个树的分类器形式')\n",
    "            w, pos = self.select_encoder(X_subset, y_subset)   # pos: sda dda cda\n",
    "\n",
    "            if max(w) == w[0]:\n",
    "                onetree = DHA_SDA(pos[0][0], pos[0][1], pos[0][2], lr=pos[0][3],\n",
    "                                  train_step=self.train_step, fault_num=self.fault_num)\n",
    "                print('第', i+1, '个分类器为DHA_SDA')\n",
    "                sda_pre, sda_circle, sda_R, sda_w, sda_b, _, _ = onetree.fit(X_subset, y_subset, i)\n",
    "                cm_estimators = confusion_matrix(y_subset, sda_pre)  # 混淆矩阵\n",
    "                para = [sda_circle, sda_R, sda_w, sda_b]\n",
    "\n",
    "            elif max(w) == w[1]:\n",
    "                onetree = DHA_DDA(pos[1][0], pos[1][1], pos[1][2], lr=pos[1][3],\n",
    "                                  train_step=self.train_step, fault_num=self.fault_num)\n",
    "                print('第', i+1, '个分类器为DHA_DDA')\n",
    "                dda_pre, dda_circle, dda_R, dda_w, dda_b, _, _ = onetree.fit(X_subset, y_subset, i)\n",
    "                cm_estimators = confusion_matrix(y_subset, dda_pre)  # 混淆矩阵\n",
    "                para = [dda_circle, dda_R, dda_w, dda_b]\n",
    "\n",
    "            else:\n",
    "                onetree = DHA_CDA(pos[2][0], pos[2][1], pos[2][2], lr=pos[2][3],\n",
    "                                  train_step=self.train_step, fault_num=self.fault_num)\n",
    "                print('第', i+1, '个编码器为DHA_CDA')\n",
    "                cda_pre, cda_circle, cda_R, cda_w, cda_b, _, _ = onetree.fit(X_subset, y_subset, i)\n",
    "                cm_estimators = confusion_matrix(y_subset, cda_pre)  # 混淆矩阵\n",
    "                para = [cda_circle, cda_R, cda_w, cda_b]\n",
    "\n",
    "\n",
    "            # 组成随机森林\n",
    "            self.forest.append(onetree)\n",
    "            self.para_forest.append(para)\n",
    "            for i in range(len(self.forest)):\n",
    "                print('第' + str(i) + '个分类器是：')\n",
    "                print(self.forest[i])\n",
    "                print('参数是：')\n",
    "                print(self.para_forest[i])\n",
    "\n",
    "            cm_estimators = cm_estimators.astype(np.float64)\n",
    "            for j in range(self.fault_num):  # 真实标签为j  :行\n",
    "                m = len(np.argwhere(y_subset == j).flatten())        # 每种标签的数量\n",
    "                label_pre = (m + 1) / (y_subset.shape[0] + y_subset.shape[1])  # 每种标签k的先验  laplace校准\n",
    "                for jj in range(self.fault_num):  # 预测标签为jj  :列\n",
    "                    cm_estimators[j][jj] = cm_estimators[j][jj] / m  # 混淆矩阵的概率形式 0.01防止分母为0\n",
    "                    self.probablity_train[i, j, jj] = label_pre * cm_estimators[j][jj]  # 后验概率\n",
    "\n",
    "        print(self.forest)\n",
    "        end_train = time.clock()\n",
    "        print(' train time: %s Seconds' % (end_train - start_train))\n",
    "\n",
    "    def predict(self, X, y):\n",
    "\n",
    "        start_test = time.clock()\n",
    "        assert len(X) > 0\n",
    "\n",
    "        # predict = np.zeros(shape=(X.shape[0], self.n_estimators))  # q个分类器的输出  E（m,q）\n",
    "        predict = []\n",
    "        final_pre = np.zeros(shape=(X.shape[0], 1))   # 最终的预测\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            _, y_pre = self.forest[i].test(X, y,\n",
    "                self.para_forest[i][0], self.para_forest[i][1], self.para_forest[i][2], self.para_forest[i][3])    # y_pre：shape：（，1）\n",
    "\n",
    "            predict.append(y_pre)\n",
    "\n",
    "\n",
    "        index_lrp = []\n",
    "        index = 0\n",
    "        for m in range(X.shape[0]):\n",
    "            max = 0\n",
    "            axis_x = y[m][0]  # 实际标签\n",
    "            for j in range(self.n_estimators):\n",
    "                axis_y = int(predict[j][m])  # 第j个编码器对第i个样本的预测标签\n",
    "                t = self.probablity_train[j][axis_x][axis_y]  # 第j个编码器的 后验概率\n",
    "                if t > max:\n",
    "                    max = t\n",
    "                    final_pre[m] = predict[j][m]\n",
    "                    index = j      # 使用的哪个分类器的分类结果\n",
    "            index_lrp.append(index)\n",
    "\n",
    "        final_pre = final_pre.astype(np.int64)\n",
    "\n",
    "\n",
    "        dir1 = './DHA_AOA_Bagging/'\n",
    "        if not os.path.exists(dir1):\n",
    "            os.mkdir(dir1)\n",
    "        picturesdir = './DHA_AOA_Bagging/bagging_result_pictures/'\n",
    "        if not os.path.exists(picturesdir):\n",
    "            os.mkdir(picturesdir)\n",
    "\n",
    "        fig = Visualization(y, final_pre, picturesdir)\n",
    "        acc = fig.plot_confusion_matrix('DHA_AOA_bagging test HAR Confusion Matrix' )  # 画混淆矩阵\n",
    "        # fig.tsne_precl(X, 'DHA_AOA_bagging Initial data')\n",
    "        # fig.tsne_af(X,  'DHA_AOA_bagging Data after classification')\n",
    "        fig.plot_roc('DHA_AOA_bagging ROC curve')  # 画roc图\n",
    "        end_test = time.clock()\n",
    "        print('test time: %s Seconds' % (end_test - start_test))\n",
    "\n",
    "        return final_pre, index_lrp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = shujuchuli.cut_samples(0)\n",
    "train_x, train_y, test_x, test_y = shujuchuli.make_datasets(data)\n",
    "\n",
    "\n",
    "suiji_encoder = Bagging_encoder(n_estimators=3,\n",
    "                                subsamples=2*train_x.shape[0]//3,\n",
    "                                subfeatures=train_x.shape[1]//2,\n",
    "                                train_step=300,\n",
    "                                aoa_size=2, aoa_iter_num=20)\n",
    "encoder_li = suiji_encoder.fit(train_x, train_y)\n",
    "\n",
    "# 模型验证\n",
    "\n",
    "rf_test_pre, index_lrp = suiji_encoder.predict(test_x, test_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_gpu]",
   "language": "python",
   "name": "conda-env-tensorflow_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
